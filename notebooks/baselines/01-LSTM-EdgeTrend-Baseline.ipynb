{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "MJouDrUr44kn",
      "metadata": {
        "id": "MJouDrUr44kn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722363573195,
          "user_tz": 240,
          "elapsed": 3845,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "import xxhash"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780a3e3d",
      "metadata": {
        "id": "780a3e3d"
      },
      "source": [
        "# Loading data from disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "pLW3HgyQ2CFP",
      "metadata": {
        "id": "pLW3HgyQ2CFP",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722363573196,
          "user_tz": 240,
          "elapsed": 2,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "def load_data(year, data_dir, percentile=0.9):\n",
        "    edges = pd.read_parquet(f'{data_dir}/{year}/{year}_edges.parquet', engine='pyarrow')\n",
        "    nodes = pd.read_parquet(f'{data_dir}/{year}/{year}_nodes.parquet', engine='pyarrow')\n",
        "    weight_threshold = edges['weight'].quantile(percentile)\n",
        "    filtered_edges = edges[edges['weight'] >= weight_threshold]\n",
        "    return filtered_edges, nodes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"gs://datasets-dev-ded86f66/benchmarks/scientific_trend_prediction/new_parquet_data\"\n",
        "years = range(1980, 2024)\n",
        "\n",
        "all_node_ids = set()\n",
        "id_to_label = {}\n",
        "for i in years:\n",
        "    _, n = load_data(i, data_dir)\n",
        "    all_node_ids = all_node_ids.union(set(n['node_id'].tolist()))\n",
        "    keys , vals = n['node_id'].tolist() , n['node_label'].tolist()\n",
        "    entries = {key: value for key, value in zip(keys, vals)}\n",
        "    id_to_label.update(entries)"
      ],
      "metadata": {
        "id": "tHTJXOlEbizF",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722363608399,
          "user_tz": 240,
          "elapsed": 35204,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "tHTJXOlEbizF",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import networkx as nx\n",
        "\n",
        "def featurizer(edges, node_ids, id_to_label):\n",
        "    label_order = ['phenotype', 'gene', 'compound']\n",
        "    label_to_index = {label: i for i, label in enumerate(label_order)}\n",
        "\n",
        "    node_features = np.zeros((len(node_ids), 3), dtype=float)\n",
        "    out_degree_count = {node: {label: 0 for label in label_order} for node in node_ids}\n",
        "\n",
        "    for src, dest in zip(edges['source_id'], edges['destination_id']):\n",
        "        dest_label = id_to_label[dest]\n",
        "        out_degree_count[src][dest_label] += 1\n",
        "\n",
        "    for i, node in enumerate(node_ids):\n",
        "        node_feature_vector = [out_degree_count[node][label] for label in label_order]\n",
        "        node_features[i] = node_feature_vector\n",
        "\n",
        "    return torch.tensor(node_features, dtype=torch.float)"
      ],
      "metadata": {
        "id": "mhrucsbpblAx",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722363608545,
          "user_tz": 240,
          "elapsed": 147,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "mhrucsbpblAx",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_ids = list(all_node_ids)\n",
        "node_id_to_index = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
        "\n",
        "graphs = []\n",
        "\n",
        "for year in years:\n",
        "    edges, _ = load_data(year, data_dir)\n",
        "    node_feature = featurizer(edges, node_ids, id_to_label)\n",
        "    edge_index = np.array([edges['source_id'].map(node_id_to_index).values,\n",
        "                           edges['destination_id'].map(node_id_to_index).values])\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "    edge_weights = torch.tensor(edges['weight'].values, dtype=torch.float)\n",
        "    g = Data(x=node_feature, edge_index=edge_index, edge_attr=edge_weights, y=edge_weights)\n",
        "    graphs.append(g)"
      ],
      "metadata": {
        "id": "OaK02gPxbo1m",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722363650094,
          "user_tz": 240,
          "elapsed": 41550,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "OaK02gPxbo1m",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a7e3451f",
      "metadata": {
        "id": "a7e3451f"
      },
      "source": [
        "# Constructing temporal graph sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "64acd77b-7d6a-4ee6-ad00-a3068e73f234",
      "metadata": {
        "id": "64acd77b-7d6a-4ee6-ad00-a3068e73f234",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722363690610,
          "user_tz": 240,
          "elapsed": 40517,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import networkx as nx\n",
        "\n",
        "def featurizer(edges, node_ids, id_to_label):\n",
        "    label_order = ['phenotype', 'gene', 'compound']\n",
        "    label_to_index = {label: i for i, label in enumerate(label_order)}\n",
        "\n",
        "    node_features = np.zeros((len(node_ids), 3), dtype=float)\n",
        "    out_degree_count = {node: {label: 0 for label in label_order} for node in node_ids}\n",
        "\n",
        "    for src, dest in zip(edges['source_id'], edges['destination_id']):\n",
        "        dest_label = id_to_label[dest]\n",
        "        out_degree_count[src][dest_label] += 1\n",
        "\n",
        "    for i, node in enumerate(node_ids):\n",
        "        node_feature_vector = [out_degree_count[node][label] for label in label_order]\n",
        "        node_features[i] = node_feature_vector\n",
        "\n",
        "    return torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "node_ids = list(all_node_ids)\n",
        "node_id_to_index = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
        "\n",
        "graphs = []\n",
        "\n",
        "for year in years:\n",
        "    edges, _ = load_data(year, data_dir)\n",
        "    node_feature = featurizer(edges, node_ids, id_to_label)\n",
        "    edge_index = np.array([edges['source_id'].map(node_id_to_index).values,\n",
        "                           edges['destination_id'].map(node_id_to_index).values])\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "    edge_weights = torch.tensor(edges['weight'].values, dtype=torch.float)\n",
        "    g = Data(x=node_feature, edge_index=edge_index, edge_attr=edge_weights, y=edge_weights)\n",
        "    graphs.append(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab6ff68a-97ae-4dfa-9331-562ef5323120",
      "metadata": {
        "id": "ab6ff68a-97ae-4dfa-9331-562ef5323120"
      },
      "source": [
        "# Normalizing edge weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "94ef828e-58f8-46cc-bc9e-10a5ef80223b",
      "metadata": {
        "id": "94ef828e-58f8-46cc-bc9e-10a5ef80223b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722363691692,
          "user_tz": 240,
          "elapsed": 182,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "def normalize_edge_weights_min_max(graph_list):\n",
        "    all_weights = []\n",
        "    for graph in graph_list:\n",
        "        all_weights.extend(graph.edge_attr.view(-1).tolist())\n",
        "\n",
        "    min_weight = min(all_weights)\n",
        "    max_weight = max(all_weights)\n",
        "    print(min_weight)\n",
        "    print(max_weight)\n",
        "    for graph in graph_list:\n",
        "        edge_attr_normalized = (graph.edge_attr - min_weight) / (max_weight - min_weight)\n",
        "        graph.edge_attr = edge_attr_normalized\n",
        "        graph.y = edge_attr_normalized\n",
        "\n",
        "    return graph_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a4a313e4-60fe-4b17-8c47-c383fc3d5968",
      "metadata": {
        "id": "a4a313e4-60fe-4b17-8c47-c383fc3d5968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722363692318,
          "user_tz": 240,
          "elapsed": 458,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "4fc14f6c-bd5f-45c9-b792-1449a52b12bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.299999952316284\n",
            "4927.39990234375\n"
          ]
        }
      ],
      "source": [
        "graphs = normalize_edge_weights_min_max(graphs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0782b4a7-1de5-4df2-9590-8ad2c476123d",
      "metadata": {
        "id": "0782b4a7-1de5-4df2-9590-8ad2c476123d",
        "executionInfo": {
          "status": "aborted",
          "timestamp": 1722363566237,
          "user_tz": 240,
          "elapsed": 6,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "print(f\"Number of graphs: {len(graphs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78df504e-a7ef-4e2e-be78-dc7718ca8e92",
      "metadata": {
        "id": "78df504e-a7ef-4e2e-be78-dc7718ca8e92"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def create_sequences(data, time_step):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - time_step - 1):\n",
        "        X.append(data[i:(i + time_step)])\n",
        "        Y.append(data[i + time_step])\n",
        "    return X, Y\n",
        "\n",
        "x, y = create_sequences(graphs, 10)\n",
        "\n",
        "split_index = int(len(x) * 0.8)\n",
        "\n",
        "x_train, x_test = x[:split_index], x[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "print(\"Size of x_train:\", len(x_train))\n",
        "print(\"Size of x_test:\", len(x_test))\n",
        "print(\"Size of y_train:\", len(y_train))\n",
        "print(\"Size of y_test:\", len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4c7b267-207a-4d69-a597-eb8bb3d806ba",
      "metadata": {
        "id": "f4c7b267-207a-4d69-a597-eb8bb3d806ba"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "def add_negative_samples(data):\n",
        "    num_pos_samples = data.edge_index.size(1)\n",
        "    num_neg_samples = num_pos_samples\n",
        "    neg_edge_index = negative_sampling(data.edge_index, num_nodes=data.num_nodes, num_neg_samples=num_neg_samples)\n",
        "    pos_weights = torch.ones(num_pos_samples, device=data.edge_index.device)\n",
        "    neg_weights = torch.zeros(num_neg_samples, device=data.edge_index.device)\n",
        "\n",
        "    data.edge_index = torch.cat([data.edge_index, neg_edge_index], dim=1)\n",
        "    data.y = torch.cat([pos_weights, neg_weights])\n",
        "    data.edge_attr = data.y\n",
        "\n",
        "    perm = torch.randperm(data.edge_index.size(1))\n",
        "\n",
        "    data.edge_index = data.edge_index[:, perm]\n",
        "    data.edge_attr = data.edge_attr[perm]\n",
        "    data.y = data.y[perm]\n",
        "\n",
        "    return data\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "    y_train[i] = add_negative_samples(y_train[i])\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    y_test[i] = add_negative_samples(y_test[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93688385-dd4b-4d36-9998-bc9e69bc51a2",
      "metadata": {
        "id": "93688385-dd4b-4d36-9998-bc9e69bc51a2"
      },
      "source": [
        "# LSTM Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57df6e0c-ad3e-4df5-9eb5-7d3dcd443721",
      "metadata": {
        "id": "57df6e0c-ad3e-4df5-9eb5-7d3dcd443721"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a68f09-e096-48a0-9a1f-0ea1ba17c494",
      "metadata": {
        "id": "c7a68f09-e096-48a0-9a1f-0ea1ba17c494"
      },
      "outputs": [],
      "source": [
        "def concatenate_edge_weights(grphs):\n",
        "    reference_edge_index = grphs[0].edge_index\n",
        "    reference_edge_attr = grphs[0].edge_attr\n",
        "\n",
        "    edge_dict = {}\n",
        "    for i in range(reference_edge_index.shape[1]):\n",
        "        edge = tuple(reference_edge_index[:, i].numpy())\n",
        "        edge_dict[edge] = [reference_edge_attr[i].item()]\n",
        "\n",
        "    for grph in grphs[1:]:\n",
        "        edge_attr = grph.edge_attr\n",
        "        edge_index = grph.edge_index\n",
        "\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            edge = tuple(edge_index[:, i].numpy())\n",
        "            if edge in edge_dict:\n",
        "                edge_dict[edge].append(edge_attr[i].item())\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    max_len = len(grphs)\n",
        "\n",
        "    concatenated_weights = []\n",
        "    for edge, weights in edge_dict.items():\n",
        "        while len(weights) < max_len:\n",
        "            weights.append(0.0)\n",
        "        concatenated_weights.append(np.array(weights))\n",
        "\n",
        "    concatenated_weights_array = np.array(concatenated_weights)\n",
        "\n",
        "    return concatenated_weights_array"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def concatenate_edge_weights(grphs):\n",
        "    reference_edge_index = grphs[0].edge_index\n",
        "    reference_edge_attr = grphs[0].edge_attr\n",
        "\n",
        "    edge_dict = {}\n",
        "    for i in range(reference_edge_index.shape[1]):\n",
        "        edge = tuple(reference_edge_index[:, i].numpy())\n",
        "        edge_dict[edge] = [reference_edge_attr[i].item()]\n",
        "\n",
        "    max_len = len(grphs)\n",
        "\n",
        "    for grph in grphs[1:]:\n",
        "        edge_attr = grph.edge_attr\n",
        "        edge_index = grph.edge_index\n",
        "\n",
        "        current_edges = {tuple(edge_index[:, i].numpy()): edge_attr[i].item() for i in range(edge_index.shape[1])}\n",
        "\n",
        "        for edge in edge_dict.keys():\n",
        "            if edge in current_edges:\n",
        "                edge_dict[edge].append(current_edges[edge])\n",
        "            else:\n",
        "                edge_dict[edge].append(0.0)\n",
        "\n",
        "    concatenated_weights = [np.array(weights) for weights in edge_dict.values()]\n",
        "    concatenated_weights_array = np.array(concatenated_weights)\n",
        "\n",
        "    return concatenated_weights_array"
      ],
      "metadata": {
        "id": "CmsT3bQ5MfjW"
      },
      "id": "CmsT3bQ5MfjW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a3e302-52d6-4113-8c35-ac4d6609e011",
      "metadata": {
        "id": "13a3e302-52d6-4113-8c35-ac4d6609e011"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "seq_data_batchs = []\n",
        "\n",
        "for i in tqdm(range(len(x_train)), desc=\"Processing batches\"):\n",
        "    weights = concatenate_edge_weights([y_train[i]] + x_train[i])\n",
        "    seq_data_batchs.append(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a77e7747-2bf2-460e-ac6e-f4b604af9491",
      "metadata": {
        "id": "a77e7747-2bf2-460e-ac6e-f4b604af9491"
      },
      "outputs": [],
      "source": [
        "k = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "779c0b6d-9797-4648-9cee-3a01ad190a11",
      "metadata": {
        "id": "779c0b6d-9797-4648-9cee-3a01ad190a11"
      },
      "outputs": [],
      "source": [
        "input_dim = k\n",
        "time_step = 10\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(time_step, input_dim)))\n",
        "model.add(LSTM(50, return_sequences=False))\n",
        "model.add(Dense(input_dim,activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "for batch in seq_data_batchs:\n",
        "    X, Y = batch[:, 1:], batch[:, 0]\n",
        "\n",
        "    for j in range(0, X.shape[0], k):\n",
        "        x, y = X[j:j+k, :], Y[j:j+k]\n",
        "\n",
        "        if len(x) < k:\n",
        "            continue\n",
        "\n",
        "        x = x.transpose(1, 0)\n",
        "        y = y.reshape(1, -1)\n",
        "\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "\n",
        "        model.fit(x, y, epochs=3, batch_size=1,verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bba1b195-eba8-4470-8e21-478d53f7ba9e",
      "metadata": {
        "id": "bba1b195-eba8-4470-8e21-478d53f7ba9e"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91293c52-4f52-4ad5-a292-49807d400872",
      "metadata": {
        "id": "91293c52-4f52-4ad5-a292-49807d400872"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "test_seq_batches = []\n",
        "\n",
        "for i in tqdm(range(len(x_test)), desc=\"Processing batches\"):\n",
        "    weights = concatenate_edge_weights([y_test[i]] + x_test[i])\n",
        "    test_seq_batches.append(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bcb6729-6869-4f8c-ac84-a42acc02abae",
      "metadata": {
        "id": "4bcb6729-6869-4f8c-ac84-a42acc02abae"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "all_true_values = []\n",
        "all_predictions = []\n",
        "\n",
        "for batch in test_seq_batches:\n",
        "    X, Y = batch[:, 1:], batch[:, 0]\n",
        "\n",
        "    for j in range(0, X.shape[0], k):\n",
        "        x, y = X[j:j+k, :], Y[j:j+k]\n",
        "\n",
        "        if len(x) < k:\n",
        "            continue\n",
        "\n",
        "        x = x.transpose(1, 0)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "        yhat = model.predict(x,verbose=0)\n",
        "        yhat = yhat.reshape(-1, 1)\n",
        "\n",
        "        all_true_values.extend(y)\n",
        "        all_predictions.extend(yhat)\n",
        "\n",
        "all_true_values = np.array(all_true_values)\n",
        "all_predictions = np.array(all_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f130388-4bbe-458d-855a-9d6af68c84fb",
      "metadata": {
        "id": "2f130388-4bbe-458d-855a-9d6af68c84fb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "binary_predictions = (all_predictions > 0.5).astype(int)\n",
        "overall_accuracy = accuracy_score(all_true_values, binary_predictions)\n",
        "\n",
        "edge_presence_indices = (all_true_values == 1)\n",
        "edge_presence_accuracy = accuracy_score(all_true_values[edge_presence_indices], binary_predictions[edge_presence_indices])\n",
        "\n",
        "edge_absence_indices = (all_true_values == 0)\n",
        "edge_absence_accuracy = accuracy_score(all_true_values[edge_absence_indices], binary_predictions[edge_absence_indices])\n",
        "\n",
        "print(\"Overall Accuracy:\", overall_accuracy)\n",
        "print(\"Edge Presence Accuracy (True value is 1):\", edge_presence_accuracy)\n",
        "print(\"Edge Absence Accuracy (True value is 0):\", edge_absence_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_confusion_matrix(total_positive, total_negative, accuracy_presence, accuracy_absence):\n",
        "    TP = accuracy_presence * total_positive\n",
        "    TN = accuracy_absence * total_negative\n",
        "    FP = total_negative - TN\n",
        "    FN = total_positive - TP\n",
        "\n",
        "    return TP, TN, FP, FN\n",
        "\n",
        "def calculate_precision_recall(TP, FP, FN):\n",
        "    if TP + FP == 0:\n",
        "        Precision = 0\n",
        "    else:\n",
        "        Precision = TP / (TP + FP)\n",
        "\n",
        "    if TP + FN == 0:\n",
        "        Recall = 0\n",
        "    else:\n",
        "        Recall = TP / (TP + FN)\n",
        "\n",
        "    return Precision, Recall\n",
        "\n",
        "total_samples = sum(len(x.y) for x in y_test)\n",
        "total_positive = total_samples // 2\n",
        "total_negative = total_positive\n",
        "accuracy_presence = edge_presence_accuracy\n",
        "accuracy_absence = edge_absence_accuracy\n",
        "\n",
        "TP, TN, FP, FN = calculate_confusion_matrix(total_positive, total_negative, accuracy_presence, accuracy_absence)\n",
        "Precision, Recall = calculate_precision_recall(TP, FP, FN)\n",
        "\n",
        "print(f\"Precision: {Precision}\")\n",
        "print(f\"Recall: {Recall}\")"
      ],
      "metadata": {
        "id": "t9eaJNkPAljh"
      },
      "id": "t9eaJNkPAljh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U9IxOPXvSu3K"
      },
      "id": "U9IxOPXvSu3K",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "lstm_baseline.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}