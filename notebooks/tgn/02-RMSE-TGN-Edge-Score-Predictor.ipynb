{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e1148181-c72b-4f7c-9463-fb63fefe3f12",
      "metadata": {
        "id": "e1148181-c72b-4f7c-9463-fb63fefe3f12"
      },
      "source": [
        "# Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dda1c9a3-46d6-4600-b615-f6698cffce2b",
      "metadata": {
        "id": "dda1c9a3-46d6-4600-b615-f6698cffce2b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722443816025,
          "user_tz": 240,
          "elapsed": 177,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def install_dependencies():\n",
        "    commands = [\n",
        "        \"sudo apt install unzip -y\",\n",
        "        \"pip install gdown\",\n",
        "        \"pip install torch\",\n",
        "        \"pip install torch-geometric\",\n",
        "        \"pip install numpy\",\n",
        "        \"pip install pandas\",\n",
        "        \"pip install scikit-learn\",\n",
        "        \"pip install xxhash\",\n",
        "        \"pip install pyarrow\",\n",
        "        \"pip install tensorflow\"\n",
        "    ]\n",
        "\n",
        "    for command in commands:\n",
        "        process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n",
        "        output, error = process.communicate()\n",
        "        if error:\n",
        "            print(f\"Error occurred: {error}\")\n",
        "        else:\n",
        "            print(f\"Output: {output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "MJouDrUr44kn",
      "metadata": {
        "id": "MJouDrUr44kn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722443820517,
          "user_tz": 240,
          "elapsed": 3977,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "import xxhash"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780a3e3d",
      "metadata": {
        "id": "780a3e3d"
      },
      "source": [
        "## Loading edge files with a percentile threshold on the edge weights. Higher percentile extracts stronger relations. This parameter can be adjusted to control the strength of trends that we want to predict for future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "pLW3HgyQ2CFP",
      "metadata": {
        "id": "pLW3HgyQ2CFP",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722443854899,
          "user_tz": 240,
          "elapsed": 34383,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_data(year, data_dir, percentile=0.9):\n",
        "    edges = pd.read_parquet(f'{data_dir}/{year}/{year}_edges.parquet', engine='pyarrow')\n",
        "    nodes = pd.read_parquet(f'{data_dir}/{year}/{year}_nodes.parquet', engine='pyarrow')\n",
        "    weight_threshold = edges['weight'].quantile(percentile)\n",
        "    filtered_edges = edges[edges['weight'] >= weight_threshold]\n",
        "    return filtered_edges, nodes\n",
        "\n",
        "data_dir = \"gs://datasets-dev-ded86f66/benchmarks/scientific_trend_prediction/new_parquet_data\"\n",
        "years = range(1980, 2024)\n",
        "\n",
        "_, nodes = load_data(1980, data_dir)\n",
        "all_node_ids = nodes['node_id'].tolist()\n",
        "\n",
        "all_node_ids = set()\n",
        "id_to_label = {}\n",
        "for i in years:\n",
        "    _, n = load_data(i, data_dir)\n",
        "    all_node_ids = all_node_ids.union(set(n['node_id'].tolist()))\n",
        "    keys , vals = n['node_id'].tolist() , n['node_label'].tolist()\n",
        "    entries = {key: value for key, value in zip(keys, vals)}\n",
        "    id_to_label.update(entries)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7e3451f",
      "metadata": {
        "id": "a7e3451f"
      },
      "source": [
        "# Constructing temporal graph sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "64acd77b-7d6a-4ee6-ad00-a3068e73f234",
      "metadata": {
        "id": "64acd77b-7d6a-4ee6-ad00-a3068e73f234",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722443900178,
          "user_tz": 240,
          "elapsed": 45280,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import networkx as nx\n",
        "\n",
        "def featurizer(edges, node_ids, id_to_label):\n",
        "    label_order = ['phenotype', 'gene', 'compound']\n",
        "    label_to_index = {label: i for i, label in enumerate(label_order)}\n",
        "\n",
        "    node_features = np.zeros((len(node_ids), 6), dtype=float)\n",
        "    out_degree_count = {node: {label: 0 for label in label_order} for node in node_ids}\n",
        "    in_degree_count = {node: {label: 0 for label in label_order} for node in node_ids}\n",
        "\n",
        "    for src, dest in zip(edges['source_id'], edges['destination_id']):\n",
        "        dest_label = id_to_label[dest]\n",
        "        src_label = id_to_label[src]\n",
        "        out_degree_count[src][dest_label] += 1\n",
        "        in_degree_count[dest][src_label] += 1\n",
        "\n",
        "    for i, node in enumerate(node_ids):\n",
        "        node_feature_vector = [out_degree_count[node][label] for label in label_order] + \\\n",
        "                              [in_degree_count[node][label] for label in label_order]\n",
        "        node_features[i] = node_feature_vector\n",
        "\n",
        "    return torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "node_ids = list(all_node_ids)\n",
        "node_id_to_index = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
        "\n",
        "graphs = []\n",
        "\n",
        "for year in years:\n",
        "    edges, _ = load_data(year, data_dir)\n",
        "    node_feature = featurizer(edges, node_ids, id_to_label)\n",
        "    edge_index = np.array([edges['source_id'].map(node_id_to_index).values,\n",
        "                           edges['destination_id'].map(node_id_to_index).values])\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "    edge_weights = torch.tensor(edges['weight'].values, dtype=torch.float)\n",
        "    g = Data(x=node_feature, edge_index=edge_index, edge_attr=edge_weights, y=edge_weights)\n",
        "    graphs.append(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab6ff68a-97ae-4dfa-9331-562ef5323120",
      "metadata": {
        "id": "ab6ff68a-97ae-4dfa-9331-562ef5323120"
      },
      "source": [
        "# Normalizing edge weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "94ef828e-58f8-46cc-bc9e-10a5ef80223b",
      "metadata": {
        "id": "94ef828e-58f8-46cc-bc9e-10a5ef80223b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722443900178,
          "user_tz": 240,
          "elapsed": 2,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "def normalize_edge_weights_min_max(graph_list):\n",
        "    all_weights = []\n",
        "    for graph in graph_list:\n",
        "        all_weights.extend(graph.edge_attr.view(-1).tolist())\n",
        "\n",
        "    min_weight = np.array(all_weights).min()\n",
        "    max_weight = np.array(all_weights).max()\n",
        "\n",
        "    for graph in graph_list:\n",
        "        edge_attr_normalized = (graph.edge_attr - min_weight) / (max_weight - min_weight)\n",
        "        graph.edge_attr = edge_attr_normalized\n",
        "        graph.y = edge_attr_normalized\n",
        "\n",
        "    return graph_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graphs = normalize_edge_weights_min_max(graphs)\n",
        "print(f\"Number of graphs: {len(graphs)}\")"
      ],
      "metadata": {
        "id": "fMVIC7NXWvzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722443900910,
          "user_tz": 240,
          "elapsed": 733,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "1a6f120c-0d5d-4e8e-8e51-a23a5504c537"
      },
      "id": "fMVIC7NXWvzW",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of graphs: 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "077331ff-6c29-431b-9b45-799e9bdd03e9",
      "metadata": {
        "id": "077331ff-6c29-431b-9b45-799e9bdd03e9"
      },
      "source": [
        "# GNN-LSTM Layer Implemetation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8d1f92a2-55c4-4d64-86f7-16ca27409369",
      "metadata": {
        "id": "8d1f92a2-55c4-4d64-86f7-16ca27409369",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722443900910,
          "user_tz": 240,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "from torch.nn import Parameter\n",
        "from torch_geometric.nn import ChebConv\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "\n",
        "class GConvLSTM(torch.nn.Module):\n",
        "    r\"\"\"An implementation of the Chebyshev Graph Convolutional Long Short Term Memory\n",
        "    Cell. For details see this paper: `\"Structured Sequence Modeling with Graph\n",
        "    Convolutional Recurrent Networks.\" <https://arxiv.org/abs/1612.07659>`_\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of input features.\n",
        "        out_channels (int): Number of output features.\n",
        "        K (int): Chebyshev filter size :math:`K`.\n",
        "        normalization (str, optional): The normalization scheme for the graph\n",
        "            Laplacian (default: :obj:`\"sym\"`):\n",
        "\n",
        "            1. :obj:`None`: No normalization\n",
        "            :math:`\\mathbf{L} = \\mathbf{D} - \\mathbf{A}`\n",
        "\n",
        "            2. :obj:`\"sym\"`: Symmetric normalization\n",
        "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\mathbf{A}\n",
        "            \\mathbf{D}^{-1/2}`\n",
        "\n",
        "            3. :obj:`\"rw\"`: Random-walk normalization\n",
        "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}`\n",
        "\n",
        "            You need to pass :obj:`lambda_max` to the :meth:`forward` method of\n",
        "            this operator in case the normalization is non-symmetric.\n",
        "            :obj:`\\lambda_max` should be a :class:`torch.Tensor` of size\n",
        "            :obj:`[num_graphs]` in a mini-batch scenario and a\n",
        "            scalar/zero-dimensional tensor when operating on single graphs.\n",
        "            You can pre-compute :obj:`lambda_max` via the\n",
        "            :class:`torch_geometric.transforms.LaplacianLambdaMax` transform.\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        K: int,\n",
        "        normalization: str = \"sym\",\n",
        "        bias: bool = True,\n",
        "    ):\n",
        "        super(GConvLSTM, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.K = K\n",
        "        self.normalization = normalization\n",
        "        self.bias = bias\n",
        "        self._create_parameters_and_layers()\n",
        "        self._set_parameters()\n",
        "\n",
        "    def _create_input_gate_parameters_and_layers(self):\n",
        "\n",
        "        self.conv_x_i = ChebConv(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.conv_h_i = ChebConv(\n",
        "            in_channels=self.out_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.w_c_i = Parameter(torch.Tensor(1, self.out_channels))\n",
        "        self.b_i = Parameter(torch.Tensor(1, self.out_channels))\n",
        "\n",
        "    def _create_forget_gate_parameters_and_layers(self):\n",
        "\n",
        "        self.conv_x_f = ChebConv(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.conv_h_f = ChebConv(\n",
        "            in_channels=self.out_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.w_c_f = Parameter(torch.Tensor(1, self.out_channels))\n",
        "        self.b_f = Parameter(torch.Tensor(1, self.out_channels))\n",
        "\n",
        "    def _create_cell_state_parameters_and_layers(self):\n",
        "\n",
        "        self.conv_x_c = ChebConv(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.conv_h_c = ChebConv(\n",
        "            in_channels=self.out_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.b_c = Parameter(torch.Tensor(1, self.out_channels))\n",
        "\n",
        "    def _create_output_gate_parameters_and_layers(self):\n",
        "\n",
        "        self.conv_x_o = ChebConv(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.conv_h_o = ChebConv(\n",
        "            in_channels=self.out_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.w_c_o = Parameter(torch.Tensor(1, self.out_channels))\n",
        "        self.b_o = Parameter(torch.Tensor(1, self.out_channels))\n",
        "\n",
        "    def _create_parameters_and_layers(self):\n",
        "        self._create_input_gate_parameters_and_layers()\n",
        "        self._create_forget_gate_parameters_and_layers()\n",
        "        self._create_cell_state_parameters_and_layers()\n",
        "        self._create_output_gate_parameters_and_layers()\n",
        "\n",
        "    def _set_parameters(self):\n",
        "        glorot(self.w_c_i)\n",
        "        glorot(self.w_c_f)\n",
        "        glorot(self.w_c_o)\n",
        "        zeros(self.b_i)\n",
        "        zeros(self.b_f)\n",
        "        zeros(self.b_c)\n",
        "        zeros(self.b_o)\n",
        "\n",
        "    def _set_hidden_state(self, X, H):\n",
        "        if H is None:\n",
        "            H = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n",
        "        return H\n",
        "\n",
        "    def _set_cell_state(self, X, C):\n",
        "        if C is None:\n",
        "            C = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n",
        "        return C\n",
        "\n",
        "    def _calculate_input_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
        "        I = self.conv_x_i(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        I = I + self.conv_h_i(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        I = I + (self.w_c_i * C)\n",
        "        I = I + self.b_i\n",
        "        I = torch.sigmoid(I)\n",
        "        return I\n",
        "\n",
        "    def _calculate_forget_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
        "        F = self.conv_x_f(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        F = F + self.conv_h_f(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        F = F + (self.w_c_f * C)\n",
        "        F = F + self.b_f\n",
        "        F = torch.sigmoid(F)\n",
        "        return F\n",
        "\n",
        "    def _calculate_cell_state(self, X, edge_index, edge_weight, H, C, I, F, lambda_max):\n",
        "        T = self.conv_x_c(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        T = T + self.conv_h_c(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        T = T + self.b_c\n",
        "        T = torch.tanh(T)\n",
        "        C = F * C + I * T\n",
        "        return C\n",
        "\n",
        "    def _calculate_output_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
        "        O = self.conv_x_o(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        O = O + self.conv_h_o(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        O = O + (self.w_c_o * C)\n",
        "        O = O + self.b_o\n",
        "        O = torch.sigmoid(O)\n",
        "        return O\n",
        "\n",
        "    def _calculate_hidden_state(self, O, C):\n",
        "        H = O * torch.tanh(C)\n",
        "        return H\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        X: torch.FloatTensor,\n",
        "        edge_index: torch.LongTensor,\n",
        "        edge_weight: torch.FloatTensor = None,\n",
        "        H: torch.FloatTensor = None,\n",
        "        C: torch.FloatTensor = None,\n",
        "        lambda_max: torch.Tensor = None,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Making a forward pass. If edge weights are not present the forward pass\n",
        "        defaults to an unweighted graph. If the hidden state and cell state\n",
        "        matrices are not present when the forward pass is called these are\n",
        "        initialized with zeros.\n",
        "\n",
        "        Arg types:\n",
        "            * **X** *(PyTorch Float Tensor)* - Node features.\n",
        "            * **edge_index** *(PyTorch Long Tensor)* - Graph edge indices.\n",
        "            * **edge_weight** *(PyTorch Long Tensor, optional)* - Edge weight vector.\n",
        "            * **H** *(PyTorch Float Tensor, optional)* - Hidden state matrix for all nodes.\n",
        "            * **C** *(PyTorch Float Tensor, optional)* - Cell state matrix for all nodes.\n",
        "            * **lambda_max** *(PyTorch Tensor, optional but mandatory if normalization is not sym)* - Largest eigenvalue of Laplacian.\n",
        "\n",
        "        Return types:\n",
        "            * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\n",
        "            * **C** *(PyTorch Float Tensor)* - Cell state matrix for all nodes.\n",
        "        \"\"\"\n",
        "        H = self._set_hidden_state(X, H)\n",
        "        C = self._set_cell_state(X, C)\n",
        "        I = self._calculate_input_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
        "        F = self._calculate_forget_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
        "        C = self._calculate_cell_state(X, edge_index, edge_weight, H, C, I, F, lambda_max)\n",
        "        O = self._calculate_output_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
        "        H = self._calculate_hidden_state(O, C)\n",
        "        return H, C"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d94cd12c",
      "metadata": {
        "id": "d94cd12c"
      },
      "source": [
        "# Temporal Link Predictor Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "rVlPlbnt5GIB",
      "metadata": {
        "id": "rVlPlbnt5GIB",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722443900910,
          "user_tz": 240,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "class TemporalGNN(torch.nn.Module):\n",
        "    def __init__(self, num_nodes, node_features, hidden_channels, output_channels):\n",
        "        super(TemporalGNN, self).__init__()\n",
        "        self.recurrent = GConvLSTM(node_features, hidden_channels, 3)\n",
        "        self.linear = torch.nn.Linear(hidden_channels, output_channels)\n",
        "        self.edge_mlp = torch.nn.Sequential(\n",
        "                torch.nn.Linear(2 * output_channels, hidden_channels),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Linear(hidden_channels, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, seq):\n",
        "        H, C = None, None\n",
        "        for i in range(len(seq)):\n",
        "            x = seq[i].x\n",
        "            edge_index = seq[i].edge_index\n",
        "            edge_attr = seq[i].edge_attr\n",
        "            H, C = self.recurrent(x, edge_index, edge_attr, H, C)\n",
        "\n",
        "        H = F.relu(H)\n",
        "        H = self.linear(H)\n",
        "        return F.log_softmax(H, dim=1)\n",
        "\n",
        "    def predict_edge_weight(self, node_embeddings, edge_index):\n",
        "        src, dst = edge_index\n",
        "        edge_features = torch.cat([node_embeddings[src], node_embeddings[dst]], dim=1)\n",
        "        probs = self.edge_mlp(edge_features)\n",
        "        probs = torch.sigmoid(probs)\n",
        "        return probs.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "187b6576",
      "metadata": {
        "id": "187b6576"
      },
      "source": [
        "# Initializing model and creating train-test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c9b4b29a-b6e8-46e6-9cf1-876266b61970",
      "metadata": {
        "id": "c9b4b29a-b6e8-46e6-9cf1-876266b61970",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722443900910,
          "user_tz": 240,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "node_dim = graphs[0].x.shape[1]\n",
        "num_nodes = graphs[0].x.shape[0]\n",
        "hidden_channels = 64\n",
        "output_channels = 64\n",
        "learning_rate = 0.0001\n",
        "epochs = 300\n",
        "time_window = 10\n",
        "weight_decay = 0.0001\n",
        "\n",
        "model = TemporalGNN(num_nodes, node_dim, hidden_channels, output_channels)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "78df504e-a7ef-4e2e-be78-dc7718ca8e92",
      "metadata": {
        "id": "78df504e-a7ef-4e2e-be78-dc7718ca8e92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722443901702,
          "user_tz": 240,
          "elapsed": 793,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "0a102515-7694-4ae7-a62f-e025c1352bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of x_train: 26\n",
            "Size of x_test: 7\n",
            "Size of y_train: 26\n",
            "Size of y_test: 7\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "\n",
        "def create_sequences(data, time_step):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - time_step - 1):\n",
        "        X.append(data[i:(i + time_step)])\n",
        "        Y.append(data[i + time_step])\n",
        "    return X, Y\n",
        "\n",
        "x, y = create_sequences(graphs, time_window)\n",
        "\n",
        "split_index = int(len(x) * 0.8)\n",
        "\n",
        "x_train, x_test = copy.deepcopy(x[:split_index]), copy.deepcopy(x[split_index:])\n",
        "y_train, y_test = copy.deepcopy(y[:split_index]), copy.deepcopy(y[split_index:])\n",
        "\n",
        "print(\"Size of x_train:\", len(x_train))\n",
        "print(\"Size of x_test:\", len(x_test))\n",
        "print(\"Size of y_train:\", len(y_train))\n",
        "print(\"Size of y_test:\", len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def create_sequences(data, time_step, forecast_length):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - time_step - forecast_length + 1):\n",
        "        X.append(data[i:i + time_step])\n",
        "        Y.append(data[i + time_step + forecast_length - 1])\n",
        "    return X, Y\n",
        "\n",
        "time_window = 10\n",
        "forecast_length = 5\n",
        "\n",
        "x, y = create_sequences(graphs, time_window, forecast_length)\n",
        "\n",
        "split_index = int(len(x) * 0.8)\n",
        "\n",
        "x_train, x_test = copy.deepcopy(x[:split_index]), copy.deepcopy(x[split_index:])\n",
        "y_train, y_test = copy.deepcopy(y[:split_index]), copy.deepcopy(y[split_index:])\n",
        "\n",
        "print(\"Size of x_train:\", len(x_train))\n",
        "print(\"Size of x_test:\", len(x_test))\n",
        "print(\"Size of y_train:\", len(y_train))\n",
        "print(\"Size of y_test:\", len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jzHRQeltlO9V",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722443901702,
          "user_tz": 240,
          "elapsed": 1,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "107f4408-ce18-4848-df6d-d7bbb9519932"
      },
      "id": "jzHRQeltlO9V",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of x_train: 24\n",
            "Size of x_test: 6\n",
            "Size of y_train: 24\n",
            "Size of y_test: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eff7b51-e82d-4be9-941e-39dee26efc4e",
      "metadata": {
        "id": "0eff7b51-e82d-4be9-941e-39dee26efc4e"
      },
      "source": [
        "# Training Loop to generate edges for N+1th graph using last N graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9dac0b90-096d-4529-944e-16ddca73fa56",
      "metadata": {
        "id": "9dac0b90-096d-4529-944e-16ddca73fa56",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1722443907686,
          "user_tz": 240,
          "elapsed": 5984,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "def add_negative_samples(data):\n",
        "    num_pos_samples = data.edge_index.size(1)\n",
        "    num_neg_samples = num_pos_samples\n",
        "    neg_edge_index = negative_sampling(data.edge_index, num_nodes=data.num_nodes, num_neg_samples=num_neg_samples)\n",
        "    neg_weights = torch.zeros(num_neg_samples, device=data.edge_index.device)\n",
        "\n",
        "    data.edge_index = torch.cat([data.edge_index, neg_edge_index], dim=1)\n",
        "    data.y = torch.cat([data.edge_attr, neg_weights])\n",
        "    data.edge_attr = data.y\n",
        "\n",
        "    perm = torch.randperm(data.edge_index.size(1))\n",
        "\n",
        "    data.edge_index = data.edge_index[:, perm]\n",
        "    data.edge_attr = data.edge_attr[perm]\n",
        "    data.y = data.y[perm]\n",
        "\n",
        "    return data\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "    y_train[i] = add_negative_samples(y_train[i])\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    y_test[i] = add_negative_samples(y_test[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8a551bd8-44b4-495f-b587-cbc84fac52a0",
      "metadata": {
        "id": "8a551bd8-44b4-495f-b587-cbc84fac52a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "executionInfo": {
          "status": "error",
          "timestamp": 1722457798121,
          "user_tz": 240,
          "elapsed": 13807854,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "4f4346bd-a2ae-449a-80ca-a3da0ae736ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.016054514368685584, Val Loss: 0.006099084780241053\n",
            "Epoch 2, Loss: 0.004341600986663252, Val Loss: 0.003088829650854071\n",
            "Epoch 3, Loss: 0.0025885256861026087, Val Loss: 0.0021317903495704136\n",
            "Epoch 4, Loss: 0.0018623252156733845, Val Loss: 0.0016131195394943159\n",
            "Epoch 5, Loss: 0.0014314284053398296, Val Loss: 0.0012799653341062367\n",
            "Epoch 6, Loss: 0.0011439187267872815, Val Loss: 0.0010496466614616413\n",
            "Epoch 7, Loss: 0.0009404007238723958, Val Loss: 0.0008826967192968974\n",
            "Epoch 8, Loss: 0.0007903309330383005, Val Loss: 0.0007573569115872184\n",
            "Epoch 9, Loss: 0.0006761534896213561, Val Loss: 0.0006606474247140189\n",
            "Epoch 10, Loss: 0.0005870875490169661, Val Loss: 0.0005843628120298187\n",
            "Epoch 11, Loss: 0.0005161993491734999, Val Loss: 0.0005230888006432602\n",
            "Epoch 12, Loss: 0.0004587934066269857, Val Loss: 0.0004730833558520923\n",
            "Epoch 13, Loss: 0.0004115947643488956, Val Loss: 0.00043172529452325154\n",
            "Epoch 14, Loss: 0.0003723004568504014, Val Loss: 0.00039711943342505646\n",
            "Epoch 15, Loss: 0.0003392270991753321, Val Loss: 0.00036786505855464685\n",
            "Epoch 16, Loss: 0.0003111178363421156, Val Loss: 0.0003429075877647847\n",
            "Epoch 17, Loss: 0.0002870199326328778, Val Loss: 0.00032144138822332025\n",
            "Epoch 18, Loss: 0.0002661997532413807, Val Loss: 0.00030284169285247725\n",
            "Epoch 19, Loss: 0.0002480849495138197, Val Loss: 0.0002866178013694783\n",
            "Epoch 20, Loss: 0.00023222323276665216, Val Loss: 0.00027238057130792487\n",
            "Epoch 21, Loss: 0.000218253900432804, Val Loss: 0.0002598174614831805\n",
            "Epoch 22, Loss: 0.00020588569774796875, Val Loss: 0.00024867533162857097\n",
            "Epoch 23, Loss: 0.00019488171589424988, Val Loss: 0.00023874741843125472\n",
            "Epoch 24, Loss: 0.00018504762677669836, Val Loss: 0.00022986329713603482\n",
            "Epoch 25, Loss: 0.00017622244134448314, Val Loss: 0.00022188135577986637\n",
            "Epoch 26, Loss: 0.00016827243719793236, Val Loss: 0.00021468372870003805\n",
            "Epoch 27, Loss: 0.00016108519230328966, Val Loss: 0.0002081709923610712\n",
            "Epoch 28, Loss: 0.00015456595610885415, Val Loss: 0.00020225902941698828\n",
            "Epoch 29, Loss: 0.00014863428865889242, Val Loss: 0.00019687654033380872\n",
            "Epoch 30, Loss: 0.00014322168196182852, Val Loss: 0.00019196225669778263\n",
            "Epoch 31, Loss: 0.00013826913436787436, Val Loss: 0.00018746364366961643\n",
            "Epoch 32, Loss: 0.00013372602218926963, Val Loss: 0.00018333529927379763\n",
            "Epoch 33, Loss: 0.0001295484765554041, Val Loss: 0.00017953801337474337\n",
            "Epoch 34, Loss: 0.00012569838812244902, Val Loss: 0.0001760375380399637\n",
            "Epoch 35, Loss: 0.00012214248605838898, Val Loss: 0.00017280403214196363\n",
            "Epoch 36, Loss: 0.00011885159498585078, Val Loss: 0.00016981115428886065\n",
            "Epoch 37, Loss: 0.00011580020873225294, Val Loss: 0.00016703588577608267\n",
            "Epoch 38, Loss: 0.0001129656602643081, Val Loss: 0.0001644579194059285\n",
            "Epoch 39, Loss: 0.00011032810471078847, Val Loss: 0.00016205921565415338\n",
            "Epoch 40, Loss: 0.00010786975083950286, Val Loss: 0.00015982364372272664\n",
            "Epoch 41, Loss: 0.00010557486893958412, Val Loss: 0.00015773711008174965\n",
            "Epoch 42, Loss: 0.00010342943460273091, Val Loss: 0.00015578670960773403\n",
            "Epoch 43, Loss: 0.0001014207703822952, Val Loss: 0.00015396110150807848\n",
            "Epoch 44, Loss: 9.953763734908232e-05, Val Loss: 0.00015224997090020528\n",
            "Epoch 45, Loss: 9.776990039730056e-05, Val Loss: 0.00015064424466496953\n",
            "Epoch 46, Loss: 9.610842001469185e-05, Val Loss: 0.00014913545358770838\n",
            "Epoch 47, Loss: 9.45449555729283e-05, Val Loss: 0.00014771624410059303\n",
            "Epoch 48, Loss: 9.307209893449908e-05, Val Loss: 0.00014637980348197743\n",
            "Epoch 49, Loss: 9.168305041384883e-05, Val Loss: 0.00014511994231725112\n",
            "Epoch 50, Loss: 9.037165000336245e-05, Val Loss: 0.00014393099263543263\n",
            "Epoch 51, Loss: 8.913231204132899e-05, Val Loss: 0.00014280795585364103\n",
            "Epoch 52, Loss: 8.795994093209931e-05, Val Loss: 0.00014174610987538472\n",
            "Epoch 53, Loss: 8.684989400838579e-05, Val Loss: 0.0001407412467718435\n",
            "Epoch 54, Loss: 8.579789967673908e-05, Val Loss: 0.00013978943873856528\n",
            "Epoch 55, Loss: 8.480003619600514e-05, Val Loss: 0.000138887160574086\n",
            "Epoch 56, Loss: 8.385276820869574e-05, Val Loss: 0.00013803110899364887\n",
            "Epoch 57, Loss: 8.295279485537321e-05, Val Loss: 0.00013721835421165451\n",
            "Epoch 58, Loss: 8.209710207059591e-05, Val Loss: 0.00013644606224261224\n",
            "Epoch 59, Loss: 8.128288178947211e-05, Val Loss: 0.00013571168043805906\n",
            "Epoch 60, Loss: 8.050754013311234e-05, Val Loss: 0.00013501282349655716\n",
            "Epoch 61, Loss: 7.976872696720723e-05, Val Loss: 0.00013434738745369637\n",
            "Epoch 62, Loss: 7.9064245862052e-05, Val Loss: 0.00013371328047166267\n",
            "Epoch 63, Loss: 7.839202695928786e-05, Val Loss: 0.0001331086350546684\n",
            "Epoch 64, Loss: 7.775020109572021e-05, Val Loss: 0.00013253178985905834\n",
            "Epoch 65, Loss: 7.713702537633556e-05, Val Loss: 0.0001319811271969229\n",
            "Epoch 66, Loss: 7.655086922871608e-05, Val Loss: 0.00013145508031205586\n",
            "Epoch 67, Loss: 7.599021182613797e-05, Val Loss: 0.00013095235529666147\n",
            "Epoch 68, Loss: 7.545363754009789e-05, Val Loss: 0.00013047159760996388\n",
            "Epoch 69, Loss: 7.493984655108459e-05, Val Loss: 0.00013001163582278727\n",
            "Epoch 70, Loss: 7.444761952986785e-05, Val Loss: 0.00012957134094904177\n",
            "Epoch 71, Loss: 7.39757732238407e-05, Val Loss: 0.00012914961310646808\n",
            "Epoch 72, Loss: 7.35232611077663e-05, Val Loss: 0.0001287455355244068\n",
            "Epoch 73, Loss: 7.308908485962699e-05, Val Loss: 0.00012835815020177202\n",
            "Epoch 74, Loss: 7.267229390587697e-05, Val Loss: 0.0001279866204034382\n",
            "Epoch 75, Loss: 7.227203044143e-05, Val Loss: 0.00012763013000949286\n",
            "Epoch 76, Loss: 7.188745030362043e-05, Val Loss: 0.00012728791868236536\n",
            "Epoch 77, Loss: 7.151778330201826e-05, Val Loss: 0.00012695927337820953\n",
            "Epoch 78, Loss: 7.116231472537038e-05, Val Loss: 0.00012664355381275527\n",
            "Epoch 79, Loss: 7.082035835992428e-05, Val Loss: 0.00012634012091439217\n",
            "Epoch 80, Loss: 7.04912593694947e-05, Val Loss: 0.00012604836592799984\n",
            "Epoch 81, Loss: 7.017439384071622e-05, Val Loss: 0.0001257677334554804\n",
            "Epoch 82, Loss: 6.986921501569061e-05, Val Loss: 0.00012549772266841805\n",
            "Epoch 83, Loss: 6.957517962291604e-05, Val Loss: 0.00012523783758903542\n",
            "Epoch 84, Loss: 6.929178395391015e-05, Val Loss: 0.0001249875834522148\n",
            "Epoch 85, Loss: 6.901852702867473e-05, Val Loss: 0.00012474656371826617\n",
            "Epoch 86, Loss: 6.875499108597675e-05, Val Loss: 0.00012451433576643467\n",
            "Epoch 87, Loss: 6.850071243510077e-05, Val Loss: 0.00012429050790766874\n",
            "Epoch 88, Loss: 6.825529650692867e-05, Val Loss: 0.0001240747199820665\n",
            "Epoch 89, Loss: 6.801837025705026e-05, Val Loss: 0.00012386661304238564\n",
            "Epoch 90, Loss: 6.77895651885289e-05, Val Loss: 0.0001236658669464911\n",
            "Epoch 91, Loss: 6.756853357122357e-05, Val Loss: 0.00012347213729905585\n",
            "Epoch 92, Loss: 6.735493889209465e-05, Val Loss: 0.00012328515246432895\n",
            "Epoch 93, Loss: 6.714847707674683e-05, Val Loss: 0.00012310461655336744\n",
            "Epoch 94, Loss: 6.694886254384376e-05, Val Loss: 0.00012293025914307995\n",
            "Epoch 95, Loss: 6.675580561932293e-05, Val Loss: 0.0001227618267876096\n",
            "Epoch 96, Loss: 6.656905316049233e-05, Val Loss: 0.00012259909150695117\n",
            "Epoch 97, Loss: 6.638833853382191e-05, Val Loss: 0.00012244178894131133\n",
            "Epoch 98, Loss: 6.621343572987826e-05, Val Loss: 0.00012228974082972854\n",
            "Epoch 99, Loss: 6.60441169202386e-05, Val Loss: 0.00012214270949092074\n",
            "Epoch 100, Loss: 6.588013957298244e-05, Val Loss: 0.00012200050211201112\n",
            "Epoch 101, Loss: 6.572130511509992e-05, Val Loss: 0.00012186292951810174\n",
            "Epoch 102, Loss: 6.556743392138742e-05, Val Loss: 0.00012172982193684827\n",
            "Epoch 103, Loss: 6.54183254482632e-05, Val Loss: 0.00012160099383133154\n",
            "Epoch 104, Loss: 6.527379961577633e-05, Val Loss: 0.00012147627300388801\n",
            "Epoch 105, Loss: 6.513368346835098e-05, Val Loss: 0.00012135552848728064\n",
            "Epoch 106, Loss: 6.499780996212697e-05, Val Loss: 0.00012123858808384587\n",
            "Epoch 107, Loss: 6.486603145579768e-05, Val Loss: 0.00012112532203900628\n",
            "Epoch 108, Loss: 6.473819788273734e-05, Val Loss: 0.00012101558604626916\n",
            "Epoch 109, Loss: 6.461417312190558e-05, Val Loss: 0.00012090928430552594\n",
            "Epoch 110, Loss: 6.44938095319958e-05, Val Loss: 0.00012080624461911309\n",
            "Epoch 111, Loss: 6.437696932456068e-05, Val Loss: 0.00012070635906032597\n",
            "Epoch 112, Loss: 6.426353987383966e-05, Val Loss: 0.00012060953182905602\n",
            "Epoch 113, Loss: 6.415340036861987e-05, Val Loss: 0.00012051565257327941\n",
            "Epoch 114, Loss: 6.404644106320727e-05, Val Loss: 0.00012042460002703592\n",
            "Epoch 115, Loss: 6.394254160113633e-05, Val Loss: 0.00012033628445351496\n",
            "Epoch 116, Loss: 6.384159284304285e-05, Val Loss: 0.00012025060156399074\n",
            "Epoch 117, Loss: 6.374349474450962e-05, Val Loss: 0.00012016746404697187\n",
            "Epoch 118, Loss: 6.364816348044162e-05, Val Loss: 0.00012008679671756302\n",
            "Epoch 119, Loss: 6.355550552446705e-05, Val Loss: 0.00012000851711491123\n",
            "Epoch 120, Loss: 6.346544023472234e-05, Val Loss: 0.0001199325270135887\n",
            "Epoch 121, Loss: 6.337786438355882e-05, Val Loss: 0.00011985876578061531\n",
            "Epoch 122, Loss: 6.329269247847454e-05, Val Loss: 0.00011978712912726526\n",
            "Epoch 123, Loss: 6.320984933457414e-05, Val Loss: 0.00011971757097247367\n",
            "Epoch 124, Loss: 6.312926598184276e-05, Val Loss: 0.00011965001249336638\n",
            "Epoch 125, Loss: 6.30508695091218e-05, Val Loss: 0.00011958440760887849\n",
            "Epoch 126, Loss: 6.297459610019966e-05, Val Loss: 0.00011952067870879546\n",
            "Epoch 127, Loss: 6.290036935752141e-05, Val Loss: 0.00011945875788417955\n",
            "Epoch 128, Loss: 6.28281134898619e-05, Val Loss: 0.00011939858207673144\n",
            "Epoch 129, Loss: 6.275778180982645e-05, Val Loss: 0.00011934011733198228\n",
            "Epoch 130, Loss: 6.268930474107037e-05, Val Loss: 0.00011928330059163272\n",
            "Epoch 131, Loss: 6.262265924306121e-05, Val Loss: 0.00011922809305057551\n",
            "Epoch 132, Loss: 6.255790306871252e-05, Val Loss: 0.00011917445954168215\n",
            "Epoch 133, Loss: 6.249485492541378e-05, Val Loss: 0.00011912233336867455\n",
            "Epoch 134, Loss: 6.243343250389444e-05, Val Loss: 0.00011907163813399772\n",
            "Epoch 135, Loss: 6.237358456928632e-05, Val Loss: 0.00011902233745786361\n",
            "Epoch 136, Loss: 6.231526261520533e-05, Val Loss: 0.00011897438889718615\n",
            "Epoch 137, Loss: 6.225841116247466e-05, Val Loss: 0.00011892774394558121\n",
            "Epoch 138, Loss: 6.220299837877974e-05, Val Loss: 0.0001188823832004952\n",
            "Epoch 139, Loss: 6.214898136628715e-05, Val Loss: 0.00011883825936820358\n",
            "Epoch 140, Loss: 6.209632404837369e-05, Val Loss: 0.00011879532515498188\n",
            "Epoch 141, Loss: 6.204498019239206e-05, Val Loss: 0.00011875357085955329\n",
            "Epoch 142, Loss: 6.19949135701366e-05, Val Loss: 0.00011871292857298006\n",
            "Epoch 143, Loss: 6.194608477017027e-05, Val Loss: 0.00011867338010536817\n",
            "Epoch 144, Loss: 6.189845923169439e-05, Val Loss: 0.00011863489756554675\n",
            "Epoch 145, Loss: 6.185199943805249e-05, Val Loss: 0.00011859744457372774\n",
            "Epoch 146, Loss: 6.180668363716298e-05, Val Loss: 0.0001185609993020383\n",
            "Epoch 147, Loss: 6.176246915856609e-05, Val Loss: 0.00011852551930739234\n",
            "Epoch 148, Loss: 6.171932939954179e-05, Val Loss: 0.00011849099125053424\n",
            "Epoch 149, Loss: 6.167723593838066e-05, Val Loss: 0.0001184573751136971\n",
            "Epoch 150, Loss: 6.163615194054728e-05, Val Loss: 0.0001184246478563485\n",
            "Epoch 151, Loss: 6.159605610870737e-05, Val Loss: 0.0001183927888632752\n",
            "Epoch 152, Loss: 6.155692297700928e-05, Val Loss: 0.00011836178115724276\n",
            "Epoch 153, Loss: 6.151871972785254e-05, Val Loss: 0.00011833157623186707\n",
            "Epoch 154, Loss: 6.14814203648469e-05, Val Loss: 0.00011830217287448856\n",
            "Epoch 155, Loss: 6.144500442436159e-05, Val Loss: 0.00011827353955595754\n",
            "Epoch 156, Loss: 6.140944462155555e-05, Val Loss: 0.00011824565081042238\n",
            "Epoch 157, Loss: 6.137472018963308e-05, Val Loss: 0.00011821849208596784\n",
            "Epoch 158, Loss: 6.13408097554687e-05, Val Loss: 0.00011819205004333828\n",
            "Epoch 159, Loss: 6.130769073327731e-05, Val Loss: 0.00011816629436604369\n",
            "Epoch 160, Loss: 6.127533909724055e-05, Val Loss: 0.00011814120443887077\n",
            "Epoch 161, Loss: 6.124373597534334e-05, Val Loss: 0.00011811677056054275\n",
            "Epoch 162, Loss: 6.121285681122875e-05, Val Loss: 0.00011809296483988874\n",
            "Epoch 163, Loss: 6.118268621927807e-05, Val Loss: 0.00011806978000095114\n",
            "Epoch 164, Loss: 6.115319911259576e-05, Val Loss: 0.0001180471833019207\n",
            "Epoch 165, Loss: 6.112438594148746e-05, Val Loss: 0.00011802516625418018\n",
            "Epoch 166, Loss: 6.109622427175054e-05, Val Loss: 0.00011800371673113356\n",
            "Epoch 167, Loss: 6.106869568611728e-05, Val Loss: 0.00011798282624416363\n",
            "Epoch 168, Loss: 6.104179616765275e-05, Val Loss: 0.00011796247902869557\n",
            "Epoch 169, Loss: 6.101556724994831e-05, Val Loss: 0.00011794266174547374\n",
            "Epoch 170, Loss: 6.098993230807537e-05, Val Loss: 0.0001179233501413061\n",
            "Epoch 171, Loss: 6.096485784231239e-05, Val Loss: 0.00011790452845161781\n",
            "Epoch 172, Loss: 6.0940333317679084e-05, Val Loss: 0.00011788618212449364\n",
            "Epoch 173, Loss: 6.0916342818018165e-05, Val Loss: 0.00011786829903333758\n",
            "Epoch 174, Loss: 6.089287292828279e-05, Val Loss: 0.00011785085977559599\n",
            "Epoch 175, Loss: 6.086990515541402e-05, Val Loss: 0.00011783385586265165\n",
            "Epoch 176, Loss: 6.084743343611384e-05, Val Loss: 0.00011781728365652573\n",
            "Epoch 177, Loss: 6.082543806466371e-05, Val Loss: 0.00011780112254200503\n",
            "Epoch 178, Loss: 6.080391176510602e-05, Val Loss: 0.00011778536524313192\n",
            "Epoch 179, Loss: 6.078284089502025e-05, Val Loss: 0.00011776999235735275\n",
            "Epoch 180, Loss: 6.0762214843634865e-05, Val Loss: 0.00011775500873530594\n",
            "Epoch 181, Loss: 6.074201231361561e-05, Val Loss: 0.000117740373146565\n",
            "Epoch 182, Loss: 6.072222527109261e-05, Val Loss: 0.00011772611954559882\n",
            "Epoch 183, Loss: 6.0702840604183926e-05, Val Loss: 0.00011771219821336369\n",
            "Epoch 184, Loss: 6.0683847095788224e-05, Val Loss: 0.0001176986079372\n",
            "Epoch 185, Loss: 6.0665233453012966e-05, Val Loss: 0.00011768534750444815\n",
            "Epoch 186, Loss: 6.06469892924603e-05, Val Loss: 0.00011767239872521411\n",
            "Epoch 187, Loss: 6.062910279069911e-05, Val Loss: 0.00011765976038683827\n",
            "Epoch 188, Loss: 6.061156454961747e-05, Val Loss: 0.000117647419150065\n",
            "Epoch 189, Loss: 6.059436479214734e-05, Val Loss: 0.00011763537258957513\n",
            "Epoch 190, Loss: 6.057749609074866e-05, Val Loss: 0.00011762360372813419\n",
            "Epoch 191, Loss: 6.0560942301890464e-05, Val Loss: 0.00011761209801382695\n",
            "Epoch 192, Loss: 6.054469978759395e-05, Val Loss: 0.00011760086514793026\n",
            "Epoch 193, Loss: 6.052875414752634e-05, Val Loss: 0.00011758988208991165\n",
            "Epoch 194, Loss: 6.051309537724592e-05, Val Loss: 0.00011757915005243073\n",
            "Epoch 195, Loss: 6.0497715442882814e-05, Val Loss: 0.00011756864356963585\n",
            "Epoch 196, Loss: 6.0482607295853086e-05, Val Loss: 0.00011755837961876144\n",
            "Epoch 197, Loss: 6.0467757748483564e-05, Val Loss: 0.00011754833637193467\n",
            "Epoch 198, Loss: 6.045316369333401e-05, Val Loss: 0.0001175385089785171\n",
            "Epoch 199, Loss: 6.043881709653457e-05, Val Loss: 0.00011752889137521076\n",
            "Epoch 200, Loss: 6.042470651361024e-05, Val Loss: 0.0001175194738607388\n",
            "Epoch 201, Loss: 6.041082391069116e-05, Val Loss: 0.00011751025037180322\n",
            "Epoch 202, Loss: 6.039716436134768e-05, Val Loss: 0.0001175012160577656\n",
            "Epoch 203, Loss: 6.038371467790663e-05, Val Loss: 0.00011749235394139153\n",
            "Epoch 204, Loss: 6.037047213188392e-05, Val Loss: 0.00011748367736193661\n",
            "Epoch 205, Loss: 6.0357427628332516e-05, Val Loss: 0.00011747515600291081\n",
            "Epoch 206, Loss: 6.034457768085607e-05, Val Loss: 0.00011746680077825052\n",
            "Epoch 207, Loss: 6.0331914255584707e-05, Val Loss: 0.00011745860804997695\n",
            "Epoch 208, Loss: 6.031943219871513e-05, Val Loss: 0.00011745055962819606\n",
            "Epoch 209, Loss: 6.0307123021630105e-05, Val Loss: 0.00011744265430024825\n",
            "Epoch 210, Loss: 6.029498172210879e-05, Val Loss: 0.00011743488600283551\n",
            "Epoch 211, Loss: 6.0283003297930314e-05, Val Loss: 0.00011742725716127704\n",
            "Epoch 212, Loss: 6.027118168579667e-05, Val Loss: 0.00011741975322365761\n",
            "Epoch 213, Loss: 6.025950907921166e-05, Val Loss: 0.00011741236812667921\n",
            "Epoch 214, Loss: 6.024798722137348e-05, Val Loss: 0.00011740510914629947\n",
            "Epoch 215, Loss: 6.0236607017335096e-05, Val Loss: 0.00011739794960400711\n",
            "Epoch 216, Loss: 6.022536642073343e-05, Val Loss: 0.00011739091011501539\n",
            "Epoch 217, Loss: 6.021425861035823e-05, Val Loss: 0.00011738396763879184\n",
            "Epoch 218, Loss: 6.0203283283044584e-05, Val Loss: 0.00011737713672725174\n",
            "Epoch 219, Loss: 6.019243649764879e-05, Val Loss: 0.00011737038463858578\n",
            "Epoch 220, Loss: 6.0181715828851644e-05, Val Loss: 0.00011736374168928403\n",
            "Epoch 221, Loss: 6.0171116047058604e-05, Val Loss: 0.0001173571642236008\n",
            "Epoch 222, Loss: 6.01606375312258e-05, Val Loss: 0.00011735069468462218\n",
            "Epoch 223, Loss: 6.015027679495688e-05, Val Loss: 0.00011734429669256012\n",
            "Epoch 224, Loss: 6.0140035505658794e-05, Val Loss: 0.0001173379799486914\n",
            "Epoch 225, Loss: 6.012990896427558e-05, Val Loss: 0.00011733173596439883\n",
            "Epoch 226, Loss: 6.0119891562256576e-05, Val Loss: 0.0001173255635270228\n",
            "Epoch 227, Loss: 6.0109990348185725e-05, Val Loss: 0.0001173194626365633\n",
            "Epoch 228, Loss: 6.010019789452296e-05, Val Loss: 0.00011731343208036075\n",
            "Epoch 229, Loss: 6.009051412547706e-05, Val Loss: 0.00011730746458245751\n",
            "Epoch 230, Loss: 6.008093691889371e-05, Val Loss: 0.0001173015565048748\n",
            "Epoch 231, Loss: 6.0071468624300906e-05, Val Loss: 0.0001172957078476126\n",
            "Epoch 232, Loss: 6.006211204597397e-05, Val Loss: 0.00011728992467396893\n",
            "Epoch 233, Loss: 6.005286096903243e-05, Val Loss: 0.00011728418758139014\n",
            "Epoch 234, Loss: 6.004371425660793e-05, Val Loss: 0.00011727851597242989\n",
            "Epoch 235, Loss: 6.00346711507882e-05, Val Loss: 0.00011727289044453452\n",
            "Epoch 236, Loss: 6.0025731651573246e-05, Val Loss: 0.00011726730978504445\n",
            "Epoch 237, Loss: 6.001689189361059e-05, Val Loss: 0.00011726178126991726\n",
            "Epoch 238, Loss: 6.000815218006513e-05, Val Loss: 0.00011725629398521657\n",
            "Epoch 239, Loss: 5.999951046457378e-05, Val Loss: 0.00011725083822966553\n",
            "Epoch 240, Loss: 5.9990966898719e-05, Val Loss: 0.00011724542855517939\n",
            "Epoch 241, Loss: 5.998251928455526e-05, Val Loss: 0.00011724005768580052\n",
            "Epoch 242, Loss: 5.9974167015752755e-05, Val Loss: 0.0001172347147075925\n",
            "Epoch 243, Loss: 5.9965904786925726e-05, Val Loss: 0.00011722941053449176\n",
            "Epoch 244, Loss: 5.9957732673865394e-05, Val Loss: 0.00011722413182724267\n",
            "Epoch 245, Loss: 5.9949649236538484e-05, Val Loss: 0.00011721887009722802\n",
            "Epoch 246, Loss: 5.994165076117497e-05, Val Loss: 0.00011721364595966104\n",
            "Epoch 247, Loss: 5.993373512562054e-05, Val Loss: 0.00011720843516134967\n",
            "Epoch 248, Loss: 5.992590255724887e-05, Val Loss: 0.00011720324861623037\n",
            "Epoch 249, Loss: 5.9918147447509305e-05, Val Loss: 0.00011719807904834549\n",
            "Epoch 250, Loss: 5.991046896269836e-05, Val Loss: 0.00011719291554375862\n",
            "Epoch 251, Loss: 5.9902860660561906e-05, Val Loss: 0.00011718776537842739\n",
            "Epoch 252, Loss: 5.989532284426483e-05, Val Loss: 0.00011718263097767097\n",
            "Epoch 253, Loss: 5.988785399798265e-05, Val Loss: 0.00011717750021489337\n",
            "Epoch 254, Loss: 5.988044896791204e-05, Val Loss: 0.00011717237187743497\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ab283c94750a>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mnode_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_edge_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-df423ba02bce>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0medge_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_attr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-aecadddb4170>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X, edge_index, edge_weight, H, C, lambda_max)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_forget_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_cell_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_output_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_hidden_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-aecadddb4170>\u001b[0m in \u001b[0;36m_calculate_output_gate\u001b[0;34m(self, X, edge_index, edge_weight, H, C, lambda_max)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_output_gate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_x_o\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mO\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_h_o\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mO\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_c_o\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/cheb_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight, batch, lambda_max)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# propagate_type: (x: Tensor, norm: Tensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mTx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTx_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/torch_geometric.nn.conv.cheb_conv_ChebConv_propagate_e3nwqpb8.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, norm, size)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# End Aggregate Forward Pre Hook #######################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         out = self.aggregate(\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, inputs, index, ptr, dim_size)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mas\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mby\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maggr\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         return self.aggr_module(inputs, index, ptr=ptr, dim_size=dim_size,\n\u001b[0m\u001b[1;32m    626\u001b[0m                                 dim=self.node_dim)\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/experimental.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_experimental_mode_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'disable_dynamic_shapes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrequired_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrequired_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/aggr/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, index, ptr, dim_size, dim, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             return super().__call__(x, index=index, ptr=ptr, dim_size=dim_size,\n\u001b[0m\u001b[1;32m    129\u001b[0m                                     dim=dim, **kwargs)\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/aggr/basic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, index, ptr, dim_size, dim)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mptr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 dim: int = -2) -> Tensor:\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/aggr/base.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, x, index, ptr, dim_size, dim, reduce)\u001b[0m\n\u001b[1;32m    180\u001b[0m             raise NotImplementedError(\n\u001b[1;32m    181\u001b[0m                 \"Aggregation requires 'index' to be specified\")\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     def to_dense_batch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/_scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sum'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for i in range(len(x_train)):\n",
        "        optimizer.zero_grad()\n",
        "        node_embeddings = model(x_train[i])\n",
        "        probs = model.predict_edge_weight(node_embeddings, y_train[i].edge_index)\n",
        "        loss = criterion(probs, y_train[i].y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(x_train)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(x_test)):\n",
        "            node_embeddings = model(x_test[i])\n",
        "            weights = model.predict_edge_weight(node_embeddings, y_test[i].edge_index)\n",
        "            val_loss = criterion(weights, y_test[i].y)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(x_test)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {avg_train_loss}, Val Loss: {avg_val_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90dc1a98-11e6-4604-bbc5-a985af3de840",
      "metadata": {
        "id": "90dc1a98-11e6-4604-bbc5-a985af3de840"
      },
      "outputs": [],
      "source": [
        "#model.load_state_dict(torch.load('temporal_gnn_weighted_model.pth'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "549dcaf8-e707-4461-8111-95ea33cc9c05",
      "metadata": {
        "id": "549dcaf8-e707-4461-8111-95ea33cc9c05"
      },
      "source": [
        "# Predicting edges of N+1th graph given last N graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e4a584-9fd5-4e74-aade-e8038d8d2a77",
      "metadata": {
        "id": "f2e4a584-9fd5-4e74-aade-e8038d8d2a77"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model.eval()\n",
        "\n",
        "all_y_true = []\n",
        "all_preds = []\n",
        "\n",
        "num_tests = len(x_test)\n",
        "\n",
        "for i in range(num_tests):\n",
        "    with torch.no_grad():\n",
        "        node_embeddings = model(x_test[i])\n",
        "        preds = model.predict_edge_weight(node_embeddings, y_test[i].edge_index)\n",
        "\n",
        "    y_true = y_test[i].y\n",
        "\n",
        "    all_y_true.append(y_true.numpy())\n",
        "    all_preds.append(preds.numpy())\n",
        "\n",
        "all_y_true = np.concatenate(all_y_true)\n",
        "all_preds = np.concatenate(all_preds)\n",
        "\n",
        "mse = mean_squared_error(all_y_true, all_preds)\n",
        "print(f'MSE: {mse}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "RMSE_TGN_Edge_Score_Predictor.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}