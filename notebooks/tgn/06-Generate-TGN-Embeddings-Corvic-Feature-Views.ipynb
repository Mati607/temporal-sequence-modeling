{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MJouDrUr44kn",
      "metadata": {
        "id": "MJouDrUr44kn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concatenating parquet files for years to create a single file"
      ],
      "metadata": {
        "id": "lA3bRtWoN7O7"
      },
      "id": "lA3bRtWoN7O7"
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"gs://datasets-dev-ded86f66/benchmarks/scientific_trend_prediction/feature_view_data\""
      ],
      "metadata": {
        "id": "tP14iBvoMURl"
      },
      "id": "tP14iBvoMURl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_data(year, data_dir, percentile=0.9):\n",
        "    edges = pd.read_parquet(f'{data_dir}/{year}/{year}_edges.parquet', engine='pyarrow')\n",
        "    nodes = pd.read_parquet(f'{data_dir}/{year}/{year}_nodes.parquet', engine='pyarrow')\n",
        "    weight_threshold = edges['weight'].quantile(percentile)\n",
        "    filtered_edges = edges[edges['weight'] >= weight_threshold]\n",
        "    return filtered_edges, nodes\n",
        "\n",
        "def generate_parquet_files():\n",
        "    data_dir = \"gs://datasets-dev-ded86f66/benchmarks/scientific_trend_prediction/parquet_data\"\n",
        "    years = range(1980, 2023)\n",
        "\n",
        "    all_edges = []\n",
        "    all_nodes = []\n",
        "\n",
        "    for year in years:\n",
        "        edges, nodes = load_data(year, data_dir)\n",
        "        edges['year'] = year\n",
        "        all_edges.append(edges)\n",
        "        all_nodes.append(nodes)\n",
        "\n",
        "    all_years_edges = pd.concat(all_edges, ignore_index=True)\n",
        "    all_year_nodes = pd.concat(all_nodes, ignore_index=True)\n",
        "\n",
        "    all_years_edges.to_parquet(f\"{path}/edges.parquet\", engine='pyarrow')\n",
        "    all_year_nodes.to_parquet(f\"{path}/nodes.parquet\", engine='pyarrow')"
      ],
      "metadata": {
        "id": "KIIM7bmqPTTX"
      },
      "id": "KIIM7bmqPTTX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating sources and feature view for trend data"
      ],
      "metadata": {
        "id": "KXYs-ltWYhFb"
      },
      "id": "KXYs-ltWYhFb"
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "node_df = pl.read_parquet(f\"{path}/nodes.parquet\").unique()\n",
        "edge_df = pl.read_parquet(f\"{path}/edges.parquet\").unique()"
      ],
      "metadata": {
        "id": "opRyGDJfYNQh"
      },
      "id": "opRyGDJfYNQh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from corvic.model import Source, Column, feature_type, FeatureView, FeatureViewEdgeTableMetadata\n",
        "from corvic import system_sqlite\n",
        "from corvic import system\n",
        "\n",
        "def construct_feature_view_by_timestamp(timestamp, client: system.Client):\n",
        "    nodes_source = (\n",
        "        Source.from_polars(\"concepts\", node_df, client)\n",
        "        .with_feature_types(\n",
        "            {\n",
        "                \"node_id\": feature_type.primary_key(),\n",
        "                \"node_label\": feature_type.text(),\n",
        "            }\n",
        "        ).as_dimension_table()\n",
        "    ).register()\n",
        "\n",
        "    edges_source = (\n",
        "            Source.from_polars(\"trends\", edge_df, client)\n",
        "            .with_feature_types(\n",
        "                {\n",
        "                    \"source_id\": feature_type.foreign_key(nodes_source.id),\n",
        "                    \"destination_id\": feature_type.foreign_key(nodes_source.id),\n",
        "                    \"weight\": feature_type.numerical(),\n",
        "                    \"year\": feature_type.numerical(),\n",
        "                }\n",
        "            )\n",
        "            .as_fact_table()\n",
        "        ).register()\n",
        "\n",
        "    feature_view = (\n",
        "            FeatureView.create(client)\n",
        "            .with_source(\n",
        "                nodes_source.id,\n",
        "                output=True\n",
        "                )\n",
        "            .with_source(\n",
        "                edges_source.id,\n",
        "                row_filter=Column(\"year\").eq(timestamp),\n",
        "                )\n",
        "            .with_all_implied_relationships()\n",
        "        )\n",
        "\n",
        "    return feature_view"
      ],
      "metadata": {
        "id": "UyiYD5ZdWfst",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1723134006482,
          "user_tz": 240,
          "elapsed": 9390,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "UyiYD5ZdWfst",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions to extract graph information from feature view to train TGN"
      ],
      "metadata": {
        "id": "EYrnwV2mY3sL"
      },
      "id": "EYrnwV2mY3sL"
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import tempfile\n",
        "\n",
        "def get_all_years_feature_views():\n",
        "    featureviews = []\n",
        "    for year in range(1980,2023):\n",
        "        with tempfile.TemporaryDirectory() as tdir:\n",
        "            client = system_sqlite.Client(Path(tdir) / \"corvic_data.sqlite3\")\n",
        "            feature_view = construct_feature_view_by_timestamp(year,client)\n",
        "            featureviews.append(feature_view)\n",
        "    return featureviews"
      ],
      "metadata": {
        "id": "CymTrtkufxfE"
      },
      "id": "CymTrtkufxfE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_edge_table(featureview):\n",
        "    table = featureview.output_edge_tables()[0]\n",
        "    edge_table_info = table.get_typed_metadata(FeatureViewEdgeTableMetadata)\n",
        "    batch = list(table.to_polars().unwrap_or_raise())[0]\n",
        "    df = batch.with_columns(\n",
        "        pl.col(edge_table_info.start_source_column_name).alias(\"source_id\"),\n",
        "        pl.col(edge_table_info.end_source_column_name).alias(\"destination_id\"),\n",
        "    ).select(\"source_id\", \"destination_id\", \"weight\")\n",
        "    return df.to_pandas()"
      ],
      "metadata": {
        "id": "aL88nZSjKU3-"
      },
      "id": "aL88nZSjKU3-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_nodes_info(featureviews):\n",
        "    node_ids = set()\n",
        "    id_to_label = {}\n",
        "    for view in featureviews:\n",
        "        nodes_source = view.sources[0]\n",
        "        nodes = pl.concat(nodes_source.table.to_polars().unwrap_or_raise())\n",
        "        node_ids = node_ids.union(set(nodes['node_id'].to_list()))\n",
        "        keys, vals = nodes['node_id'].to_list(), nodes['node_label'].to_list()\n",
        "        entries = {key: value for key, value in zip(keys, vals)}\n",
        "        id_to_label.update(entries)\n",
        "    node_ids = sorted(list(node_ids))\n",
        "    return node_ids, id_to_label"
      ],
      "metadata": {
        "id": "v5So0bZTK7pV"
      },
      "id": "v5So0bZTK7pV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def featureview_to_graph(featureview, node_ids, id_to_label):\n",
        "    node_id_to_index = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
        "    edges = get_edge_table(featureview)\n",
        "    node_feature = featurizer(edges, node_ids, id_to_label)\n",
        "    edge_index = np.array([edges['source_id'].map(node_id_to_index).values,\n",
        "                           edges['destination_id'].map(node_id_to_index).values])\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "    edge_weights = torch.tensor(edges['weight'].values, dtype=torch.float)\n",
        "    graph = Data(x=node_feature, edge_index=edge_index, edge_attr=edge_weights, y=edge_weights)\n",
        "    return graph"
      ],
      "metadata": {
        "id": "4RDzSYWhOsBO"
      },
      "id": "4RDzSYWhOsBO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def featurizer(edges, node_ids, id_to_label):\n",
        "    label_order = ['phenotype', 'gene', 'compound']\n",
        "    label_to_index = {label: i for i, label in enumerate(label_order)}\n",
        "\n",
        "    node_features = np.zeros((len(node_ids), 3), dtype=float)\n",
        "    out_degree_count = {node: {label: 0 for label in label_order} for node in node_ids}\n",
        "\n",
        "    for src, dest in zip(edges['source_id'], edges['destination_id']):\n",
        "        dest_label = id_to_label[dest]\n",
        "        out_degree_count[src][dest_label] += 1\n",
        "\n",
        "    for i, node in enumerate(node_ids):\n",
        "        node_feature_vector = [out_degree_count[node][label] for label in label_order]\n",
        "        node_features[i] = node_feature_vector\n",
        "\n",
        "    return torch.tensor(node_features, dtype=torch.float)"
      ],
      "metadata": {
        "id": "FMIC4MIuNQAO"
      },
      "id": "FMIC4MIuNQAO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featureviews = get_all_years_feature_views()\n",
        "node_ids, id_to_label = get_all_nodes_info(featureviews)"
      ],
      "metadata": {
        "id": "8_OWgDuUNrrc"
      },
      "id": "8_OWgDuUNrrc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graphs = []\n",
        "for view in featureviews:\n",
        "    graph = featureview_to_graph(view,node_ids,id_to_label)\n",
        "    graphs.append(graph)"
      ],
      "metadata": {
        "id": "d7Ge1pMrQk3O"
      },
      "id": "d7Ge1pMrQk3O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TGN Network"
      ],
      "metadata": {
        "id": "uTDOIVaqYMmb"
      },
      "id": "uTDOIVaqYMmb"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Parameter\n",
        "from torch_geometric.nn import ChebConv\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "\n",
        "class GConvLSTM(torch.nn.Module):\n",
        "    r\"\"\"An implementation of the Chebyshev Graph Convolutional Long Short Term Memory\n",
        "    Cell. For details see this paper: `\"Structured Sequence Modeling with Graph\n",
        "    Convolutional Recurrent Networks.\" <https://arxiv.org/abs/1612.07659>`_\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of input features.\n",
        "        out_channels (int): Number of output features.\n",
        "        K (int): Chebyshev filter size :math:`K`.\n",
        "        normalization (str, optional): The normalization scheme for the graph\n",
        "            Laplacian (default: :obj:`\"sym\"`):\n",
        "\n",
        "            1. :obj:`None`: No normalization\n",
        "            :math:`\\mathbf{L} = \\mathbf{D} - \\mathbf{A}`\n",
        "\n",
        "            2. :obj:`\"sym\"`: Symmetric normalization\n",
        "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\mathbf{A}\n",
        "            \\mathbf{D}^{-1/2}`\n",
        "\n",
        "            3. :obj:`\"rw\"`: Random-walk normalization\n",
        "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}`\n",
        "\n",
        "            You need to pass :obj:`lambda_max` to the :meth:`forward` method of\n",
        "            this operator in case the normalization is non-symmetric.\n",
        "            :obj:`\\lambda_max` should be a :class:`torch.Tensor` of size\n",
        "            :obj:`[num_graphs]` in a mini-batch scenario and a\n",
        "            scalar/zero-dimensional tensor when operating on single graphs.\n",
        "            You can pre-compute :obj:`lambda_max` via the\n",
        "            :class:`torch_geometric.transforms.LaplacianLambdaMax` transform.\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        K: int,\n",
        "        normalization: str = \"sym\",\n",
        "        bias: bool = True,\n",
        "    ):\n",
        "        super(GConvLSTM, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.K = K\n",
        "        self.normalization = normalization\n",
        "        self.bias = bias\n",
        "        self._create_parameters_and_layers()\n",
        "        self._set_parameters()\n",
        "\n",
        "    def _create_input_gate_parameters_and_layers(self):\n",
        "\n",
        "        self.conv_x_i = ChebConv(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.conv_h_i = ChebConv(\n",
        "            in_channels=self.out_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.w_c_i = Parameter(torch.Tensor(1, self.out_channels))\n",
        "        self.b_i = Parameter(torch.Tensor(1, self.out_channels))\n",
        "\n",
        "    def _create_forget_gate_parameters_and_layers(self):\n",
        "\n",
        "        self.conv_x_f = ChebConv(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.conv_h_f = ChebConv(\n",
        "            in_channels=self.out_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.w_c_f = Parameter(torch.Tensor(1, self.out_channels))\n",
        "        self.b_f = Parameter(torch.Tensor(1, self.out_channels))\n",
        "\n",
        "    def _create_cell_state_parameters_and_layers(self):\n",
        "\n",
        "        self.conv_x_c = ChebConv(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.conv_h_c = ChebConv(\n",
        "            in_channels=self.out_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.b_c = Parameter(torch.Tensor(1, self.out_channels))\n",
        "\n",
        "    def _create_output_gate_parameters_and_layers(self):\n",
        "\n",
        "        self.conv_x_o = ChebConv(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.conv_h_o = ChebConv(\n",
        "            in_channels=self.out_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.w_c_o = Parameter(torch.Tensor(1, self.out_channels))\n",
        "        self.b_o = Parameter(torch.Tensor(1, self.out_channels))\n",
        "\n",
        "    def _create_parameters_and_layers(self):\n",
        "        self._create_input_gate_parameters_and_layers()\n",
        "        self._create_forget_gate_parameters_and_layers()\n",
        "        self._create_cell_state_parameters_and_layers()\n",
        "        self._create_output_gate_parameters_and_layers()\n",
        "\n",
        "    def _set_parameters(self):\n",
        "        glorot(self.w_c_i)\n",
        "        glorot(self.w_c_f)\n",
        "        glorot(self.w_c_o)\n",
        "        zeros(self.b_i)\n",
        "        zeros(self.b_f)\n",
        "        zeros(self.b_c)\n",
        "        zeros(self.b_o)\n",
        "\n",
        "    def _set_hidden_state(self, X, H):\n",
        "        if H is None:\n",
        "            H = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n",
        "        return H\n",
        "\n",
        "    def _set_cell_state(self, X, C):\n",
        "        if C is None:\n",
        "            C = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n",
        "        return C\n",
        "\n",
        "    def _calculate_input_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
        "        I = self.conv_x_i(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        I = I + self.conv_h_i(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        I = I + (self.w_c_i * C)\n",
        "        I = I + self.b_i\n",
        "        I = torch.sigmoid(I)\n",
        "        return I\n",
        "\n",
        "    def _calculate_forget_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
        "        F = self.conv_x_f(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        F = F + self.conv_h_f(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        F = F + (self.w_c_f * C)\n",
        "        F = F + self.b_f\n",
        "        F = torch.sigmoid(F)\n",
        "        return F\n",
        "\n",
        "    def _calculate_cell_state(self, X, edge_index, edge_weight, H, C, I, F, lambda_max):\n",
        "        T = self.conv_x_c(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        T = T + self.conv_h_c(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        T = T + self.b_c\n",
        "        T = torch.tanh(T)\n",
        "        C = F * C + I * T\n",
        "        return C\n",
        "\n",
        "    def _calculate_output_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
        "        O = self.conv_x_o(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        O = O + self.conv_h_o(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        O = O + (self.w_c_o * C)\n",
        "        O = O + self.b_o\n",
        "        O = torch.sigmoid(O)\n",
        "        return O\n",
        "\n",
        "    def _calculate_hidden_state(self, O, C):\n",
        "        H = O * torch.tanh(C)\n",
        "        return H\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        X: torch.FloatTensor,\n",
        "        edge_index: torch.LongTensor,\n",
        "        edge_weight: torch.FloatTensor = None,\n",
        "        H: torch.FloatTensor = None,\n",
        "        C: torch.FloatTensor = None,\n",
        "        lambda_max: torch.Tensor = None,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Making a forward pass. If edge weights are not present the forward pass\n",
        "        defaults to an unweighted graph. If the hidden state and cell state\n",
        "        matrices are not present when the forward pass is called these are\n",
        "        initialized with zeros.\n",
        "\n",
        "        Arg types:\n",
        "            * **X** *(PyTorch Float Tensor)* - Node features.\n",
        "            * **edge_index** *(PyTorch Long Tensor)* - Graph edge indices.\n",
        "            * **edge_weight** *(PyTorch Long Tensor, optional)* - Edge weight vector.\n",
        "            * **H** *(PyTorch Float Tensor, optional)* - Hidden state matrix for all nodes.\n",
        "            * **C** *(PyTorch Float Tensor, optional)* - Cell state matrix for all nodes.\n",
        "            * **lambda_max** *(PyTorch Tensor, optional but mandatory if normalization is not sym)* - Largest eigenvalue of Laplacian.\n",
        "\n",
        "        Return types:\n",
        "            * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\n",
        "            * **C** *(PyTorch Float Tensor)* - Cell state matrix for all nodes.\n",
        "        \"\"\"\n",
        "        H = self._set_hidden_state(X, H)\n",
        "        C = self._set_cell_state(X, C)\n",
        "        I = self._calculate_input_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
        "        F = self._calculate_forget_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
        "        C = self._calculate_cell_state(X, edge_index, edge_weight, H, C, I, F, lambda_max)\n",
        "        O = self._calculate_output_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
        "        H = self._calculate_hidden_state(O, C)\n",
        "        return H, C"
      ],
      "metadata": {
        "id": "gjpyQzARSCte"
      },
      "id": "gjpyQzARSCte",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalGNN(torch.nn.Module):\n",
        "    def __init__(self, num_nodes, node_features, hidden_channels, output_channels):\n",
        "        super(TemporalGNN, self).__init__()\n",
        "        self.recurrent = GConvLSTM(node_features, hidden_channels, 3)\n",
        "        self.linear = torch.nn.Linear(hidden_channels, output_channels)\n",
        "        self.edge_mlp = torch.nn.Sequential(\n",
        "                torch.nn.Linear(2 * output_channels, hidden_channels),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Linear(hidden_channels, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, seq):\n",
        "        H, C = None, None\n",
        "        for i in range(len(seq)):\n",
        "            x = seq[i].x\n",
        "            edge_index = seq[i].edge_index\n",
        "            edge_attr = seq[i].edge_attr\n",
        "            H, C = self.recurrent(x, edge_index, edge_attr, H, C)\n",
        "\n",
        "        H = F.relu(H)\n",
        "        H = self.linear(H)\n",
        "        return F.log_softmax(H, dim=1)\n",
        "\n",
        "    def predict_edge_weight(self, node_embeddings, edge_index):\n",
        "        src, dst = edge_index\n",
        "        edge_features = torch.cat([node_embeddings[src], node_embeddings[dst]], dim=1)\n",
        "        probs = self.edge_mlp(edge_features)\n",
        "        probs = torch.sigmoid(probs)\n",
        "        return probs.squeeze()\n",
        "\n",
        "    def get_edge_embeddings(self, node_embeddings, edge_index):\n",
        "        src, dst = edge_index\n",
        "        edge_features = torch.cat([node_embeddings[src], node_embeddings[dst]], dim=1)\n",
        "        return edge_features"
      ],
      "metadata": {
        "id": "OsxcIfm9SsEI"
      },
      "id": "OsxcIfm9SsEI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_dim = graphs[0].x.shape[1]\n",
        "num_nodes = graphs[0].x.shape[0]\n",
        "hidden_channels = 64\n",
        "output_channels = 64\n",
        "learning_rate = 0.0001\n",
        "epochs = 30\n",
        "time_window = 10\n",
        "weight_decay = 0.0001\n",
        "\n",
        "model = TemporalGNN(num_nodes, node_dim, hidden_channels, output_channels)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "criterion = torch.nn.BCELoss()"
      ],
      "metadata": {
        "id": "8yQXDtRiSvzQ"
      },
      "id": "8yQXDtRiSvzQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop if you want to train the TGN from scratch"
      ],
      "metadata": {
        "id": "W9dwAQyRX_or"
      },
      "id": "W9dwAQyRX_or"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "\n",
        "def create_sequences(data, time_step):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - time_step - 1):\n",
        "        X.append(data[i:(i + time_step)])\n",
        "        Y.append(data[i + time_step])\n",
        "    return X, Y\n",
        "\n",
        "x, y = create_sequences(graphs, time_window)\n",
        "\n",
        "split_index = int(len(x) * 0.8)\n",
        "\n",
        "x_train, x_test = copy.deepcopy(x[:split_index]), copy.deepcopy(x[split_index:])\n",
        "y_train, y_test = copy.deepcopy(y[:split_index]), copy.deepcopy(y[split_index:])\n",
        "\n",
        "print(\"Size of x_train:\", len(x_train))\n",
        "print(\"Size of x_test:\", len(x_test))\n",
        "print(\"Size of y_train:\", len(y_train))\n",
        "print(\"Size of y_test:\", len(y_test))"
      ],
      "metadata": {
        "id": "SQJaWizmSypc"
      },
      "id": "SQJaWizmSypc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "def add_negative_samples(data):\n",
        "    num_pos_samples = data.edge_index.size(1)\n",
        "    num_neg_samples = num_pos_samples\n",
        "    neg_edge_index = negative_sampling(data.edge_index, num_nodes=data.num_nodes, num_neg_samples=num_neg_samples)\n",
        "    pos_weights = torch.ones(num_pos_samples, device=data.edge_index.device)\n",
        "    neg_weights = torch.zeros(num_neg_samples, device=data.edge_index.device)\n",
        "\n",
        "    data.edge_index = torch.cat([data.edge_index, neg_edge_index], dim=1)\n",
        "    data.y = torch.cat([pos_weights, neg_weights])\n",
        "    data.edge_attr = torch.cat([data.edge_attr, neg_weights])\n",
        "\n",
        "    perm = torch.randperm(data.edge_index.size(1))\n",
        "\n",
        "    data.edge_index = data.edge_index[:, perm]\n",
        "    data.edge_attr = data.edge_attr[perm]\n",
        "    data.y = data.y[perm]\n",
        "\n",
        "    return data\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "    y_train[i] = add_negative_samples(y_train[i])\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    y_test[i] = add_negative_samples(y_test[i])"
      ],
      "metadata": {
        "id": "LgVFkWWiS1K4"
      },
      "id": "LgVFkWWiS1K4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "TRAIN = True\n",
        "\n",
        "if TRAIN:\n",
        "  train_losses = []\n",
        "  val_accuracies_combined = []\n",
        "  val_accuracies_presence = []\n",
        "  val_accuracies_absence = []\n",
        "\n",
        "  model.train()\n",
        "  for epoch in range(epochs):\n",
        "      total_loss = 0\n",
        "      model.train()\n",
        "      for i in range(len(x_train)):\n",
        "          optimizer.zero_grad()\n",
        "          node_embeddings = model(x_train[i])\n",
        "          probs = model.predict_edge_weight(node_embeddings, y_train[i].edge_index)\n",
        "          loss = criterion(probs, y_train[i].y)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      avg_train_loss = total_loss / len(x_train)\n",
        "      train_losses.append(avg_train_loss)\n",
        "\n",
        "      # Validation\n",
        "      model.eval()\n",
        "      total_correct_presence = 0\n",
        "      total_correct_absence = 0\n",
        "      total_presence = 0\n",
        "      total_absence = 0\n",
        "      total_correct = 0\n",
        "      total_predictions = 0\n",
        "      with torch.no_grad():\n",
        "          for i in range(len(x_test)):\n",
        "              node_embeddings = model(x_test[i])\n",
        "              probs = model.predict_edge_weight(node_embeddings, y_test[i].edge_index)\n",
        "              predictions = (probs > 0.5).int()\n",
        "\n",
        "              correct_presence = ((predictions == y_test[i].y) & (y_test[i].y == 1)).sum().item()\n",
        "              correct_absence = ((predictions == y_test[i].y) & (y_test[i].y == 0)).sum().item()\n",
        "\n",
        "              total_correct_presence += correct_presence\n",
        "              total_correct_absence += correct_absence\n",
        "\n",
        "              total_presence += (y_test[i].y == 1).sum().item()\n",
        "              total_absence += (y_test[i].y == 0).sum().item()\n",
        "\n",
        "              total_correct += (predictions == y_test[i].y).sum().item()\n",
        "              total_predictions += y_test[i].y.size(0)\n",
        "\n",
        "      val_accuracy_presence = total_correct_presence / total_presence if total_presence > 0 else 0\n",
        "      val_accuracy_absence = total_correct_absence / total_absence if total_absence > 0 else 0\n",
        "      val_accuracy_combined = total_correct / total_predictions if total_predictions > 0 else 0\n",
        "\n",
        "      val_accuracies_presence.append(val_accuracy_presence)\n",
        "      val_accuracies_absence.append(val_accuracy_absence)\n",
        "      val_accuracies_combined.append(val_accuracy_combined)\n",
        "\n",
        "      print(f'Epoch {epoch+1}, Loss: {avg_train_loss:.4f}, Val Accuracy (Combined): {val_accuracy_combined:.4f}, Val Accuracy (Presence): {val_accuracy_presence:.4f}, Val Accuracy (Absence): {val_accuracy_absence:.4f}')"
      ],
      "metadata": {
        "id": "EFXv7pbKS3bF"
      },
      "id": "EFXv7pbKS3bF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing model weights to gs bucket"
      ],
      "metadata": {
        "id": "z2MdgnTVX0aJ"
      },
      "id": "z2MdgnTVX0aJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import gcsfs\n",
        "\n",
        "if TRAIN:\n",
        "  fs = gcsfs.GCSFileSystem()\n",
        "  with fs.open('gs://datasets-dev-ded86f66/benchmarks/scientific_trend_prediction/model_weights/new_tgn_model.pth', 'wb') as f:\n",
        "      torch.save(model.state_dict(), f)"
      ],
      "metadata": {
        "id": "XuxSpaxQUuPE"
      },
      "id": "XuxSpaxQUuPE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating TGN embeddings and storing to gs storage"
      ],
      "metadata": {
        "id": "rDISb9VkXvVI"
      },
      "id": "rDISb9VkXvVI"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "path = 'gs://datasets-dev-ded86f66/benchmarks/scientific_trend_prediction/model_weights/TGN_90th_precentile_checkpoint.pth'\n",
        "fs = gcsfs.GCSFileSystem()\n",
        "\n",
        "with fs.open(path, 'rb') as f:\n",
        "    state_dict = torch.load(f)\n",
        "\n",
        "model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "1twShqrXVAGa"
      },
      "id": "1twShqrXVAGa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Writing embeddings to gs bucket"
      ],
      "metadata": {
        "id": "eF28072oXqF9"
      },
      "id": "eF28072oXqF9"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import io\n",
        "from google.cloud import storage\n",
        "\n",
        "def upload_tensor_to_gcs(tensor, bucket_name, destination_blob_name):\n",
        "    \"\"\"Uploads a tensor to the GCS bucket.\"\"\"\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    buffer = io.BytesIO()\n",
        "    torch.save(tensor, buffer)\n",
        "    buffer.seek(0)\n",
        "\n",
        "    blob.upload_from_file(buffer, content_type='application/octet-stream')\n",
        "\n",
        "bucket_name = 'datasets-dev-ded86f66'\n",
        "prefix = 'benchmarks/scientific_trend_prediction/Tgn_embeddings'\n",
        "\n",
        "for year in range(len(graphs)):\n",
        "    node_embeddings = model([graphs[year]])\n",
        "    gcs_file = f'{prefix}/{1980 + year}.pt'\n",
        "\n",
        "    upload_tensor_to_gcs(node_embeddings, bucket_name, gcs_file)\n",
        "\n",
        "    print(f'Saved embeddings for year {1980 + year} to {gcs_file}')"
      ],
      "metadata": {
        "id": "Oa-rVBvYVCN1"
      },
      "id": "Oa-rVBvYVCN1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Generte_TGN_embeddings_with_Corvic_feature_views.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}