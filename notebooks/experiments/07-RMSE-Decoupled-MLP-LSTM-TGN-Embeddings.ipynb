{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MJouDrUr44kn",
      "metadata": {
        "id": "MJouDrUr44kn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "import xxhash"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780a3e3d",
      "metadata": {
        "id": "780a3e3d"
      },
      "source": [
        "## Loading edge files with a percentile threshold on the edge weights. Higher percentile extracts stronger relations. This parameter can be adjusted to control the strength of trends that we want to predict for future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b6e620a-ef2d-416f-b3e0-f404c3ebc883",
      "metadata": {
        "id": "0b6e620a-ef2d-416f-b3e0-f404c3ebc883"
      },
      "outputs": [],
      "source": [
        "def load_data(year, data_dir, percentile=0.9):\n",
        "    edges = pd.read_parquet(f'{data_dir}/{year}/{year}_edges.parquet', engine='pyarrow')\n",
        "    nodes = pd.read_parquet(f'{data_dir}/{year}/{year}_nodes.parquet', engine='pyarrow')\n",
        "    weight_threshold = edges['weight'].quantile(percentile)\n",
        "    filtered_edges = edges[edges['weight'] >= weight_threshold]\n",
        "    return filtered_edges, nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pLW3HgyQ2CFP",
      "metadata": {
        "id": "pLW3HgyQ2CFP"
      },
      "outputs": [],
      "source": [
        "data_dir = \"gs://datasets-dev-ded86f66/benchmarks/scientific_trend_prediction/new_parquet_data\"\n",
        "years = range(1980, 2024)\n",
        "\n",
        "all_node_ids = set()\n",
        "id_to_label = {}\n",
        "for i in years:\n",
        "    _, n = load_data(i, data_dir)\n",
        "    all_node_ids = all_node_ids.union(set(n['node_id'].tolist()))\n",
        "    keys , vals = n['node_id'].tolist() , n['node_label'].tolist()\n",
        "    entries = {key: value for key, value in zip(keys, vals)}\n",
        "    id_to_label.update(entries)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7e3451f",
      "metadata": {
        "id": "a7e3451f"
      },
      "source": [
        "## Extracting node features as out degree by type and converting each year data into a graph object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a76af68-c898-4c2e-9d2e-26e6bf781a0d",
      "metadata": {
        "id": "5a76af68-c898-4c2e-9d2e-26e6bf781a0d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import networkx as nx\n",
        "\n",
        "def featurizer(edges, node_ids, id_to_label):\n",
        "    label_order = ['phenotype', 'gene', 'compound']\n",
        "    label_to_index = {label: i for i, label in enumerate(label_order)}\n",
        "\n",
        "    node_features = np.zeros((len(node_ids), 3), dtype=float)\n",
        "    out_degree_count = {node: {label: 0 for label in label_order} for node in node_ids}\n",
        "\n",
        "    for src, dest in zip(edges['source_id'], edges['destination_id']):\n",
        "        dest_label = id_to_label[dest]\n",
        "        out_degree_count[src][dest_label] += 1\n",
        "\n",
        "    for i, node in enumerate(node_ids):\n",
        "        node_feature_vector = [out_degree_count[node][label] for label in label_order]\n",
        "        node_features[i] = node_feature_vector\n",
        "\n",
        "    return torch.tensor(node_features, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64acd77b-7d6a-4ee6-ad00-a3068e73f234",
      "metadata": {
        "id": "64acd77b-7d6a-4ee6-ad00-a3068e73f234"
      },
      "outputs": [],
      "source": [
        "node_ids = list(all_node_ids)\n",
        "node_id_to_index = {node_id: idx for idx, node_id in enumerate(node_ids)}\n",
        "\n",
        "graphs = []\n",
        "\n",
        "for year in years:\n",
        "    edges, _ = load_data(year, data_dir)\n",
        "    node_feature = featurizer(edges, node_ids, id_to_label)\n",
        "    edge_index = np.array([edges['source_id'].map(node_id_to_index).values,\n",
        "                           edges['destination_id'].map(node_id_to_index).values])\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "    edge_weights = torch.tensor(edges['weight'].values, dtype=torch.float)\n",
        "    g = Data(x=node_feature, edge_index=edge_index, edge_attr=edge_weights, y=edge_weights)\n",
        "    graphs.append(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalizing edge weights"
      ],
      "metadata": {
        "id": "NnQTSAovDoJ5"
      },
      "id": "NnQTSAovDoJ5"
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_edge_weights_min_max(graph_list):\n",
        "    all_weights = []\n",
        "    for graph in graph_list:\n",
        "        all_weights.extend(graph.edge_attr.view(-1).tolist())\n",
        "\n",
        "    min_weight = np.array(all_weights).min()\n",
        "    max_weight = np.array(all_weights).max()\n",
        "\n",
        "    for graph in graph_list:\n",
        "        edge_attr_normalized = (graph.edge_attr - min_weight) / (max_weight - min_weight)\n",
        "        graph.edge_attr = edge_attr_normalized\n",
        "        graph.y = edge_attr_normalized\n",
        "\n",
        "    return graph_list"
      ],
      "metadata": {
        "id": "27cJorEcDk4N"
      },
      "id": "27cJorEcDk4N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0782b4a7-1de5-4df2-9590-8ad2c476123d",
      "metadata": {
        "id": "0782b4a7-1de5-4df2-9590-8ad2c476123d"
      },
      "outputs": [],
      "source": [
        "graphs = normalize_edge_weights_min_max(graphs)\n",
        "print(f\"Number of graphs: {len(graphs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "077331ff-6c29-431b-9b45-799e9bdd03e9",
      "metadata": {
        "id": "077331ff-6c29-431b-9b45-799e9bdd03e9"
      },
      "source": [
        "## GNN-LSTM Layer Implemetation from torch-geometric temporal library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d1f92a2-55c4-4d64-86f7-16ca27409369",
      "metadata": {
        "id": "8d1f92a2-55c4-4d64-86f7-16ca27409369"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Parameter\n",
        "from torch_geometric.nn import ChebConv\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "\n",
        "class GConvLSTM(torch.nn.Module):\n",
        "    r\"\"\"An implementation of the Chebyshev Graph Convolutional Long Short Term Memory\n",
        "    Cell. For details see this paper: `\"Structured Sequence Modeling with Graph\n",
        "    Convolutional Recurrent Networks.\" <https://arxiv.org/abs/1612.07659>`_\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of input features.\n",
        "        out_channels (int): Number of output features.\n",
        "        K (int): Chebyshev filter size :math:`K`.\n",
        "        normalization (str, optional): The normalization scheme for the graph\n",
        "            Laplacian (default: :obj:`\"sym\"`):\n",
        "\n",
        "            1. :obj:`None`: No normalization\n",
        "            :math:`\\mathbf{L} = \\mathbf{D} - \\mathbf{A}`\n",
        "\n",
        "            2. :obj:`\"sym\"`: Symmetric normalization\n",
        "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1/2} \\mathbf{A}\n",
        "            \\mathbf{D}^{-1/2}`\n",
        "\n",
        "            3. :obj:`\"rw\"`: Random-walk normalization\n",
        "            :math:`\\mathbf{L} = \\mathbf{I} - \\mathbf{D}^{-1} \\mathbf{A}`\n",
        "\n",
        "            You need to pass :obj:`lambda_max` to the :meth:`forward` method of\n",
        "            this operator in case the normalization is non-symmetric.\n",
        "            :obj:`\\lambda_max` should be a :class:`torch.Tensor` of size\n",
        "            :obj:`[num_graphs]` in a mini-batch scenario and a\n",
        "            scalar/zero-dimensional tensor when operating on single graphs.\n",
        "            You can pre-compute :obj:`lambda_max` via the\n",
        "            :class:`torch_geometric.transforms.LaplacianLambdaMax` transform.\n",
        "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
        "            an additive bias. (default: :obj:`True`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        K: int,\n",
        "        normalization: str = \"sym\",\n",
        "        bias: bool = True,\n",
        "    ):\n",
        "        super(GConvLSTM, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.K = K\n",
        "        self.normalization = normalization\n",
        "        self.bias = bias\n",
        "        self._create_parameters_and_layers()\n",
        "        self._set_parameters()\n",
        "\n",
        "    def _create_input_gate_parameters_and_layers(self):\n",
        "\n",
        "        self.conv_x_i = ChebConv(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.conv_h_i = ChebConv(\n",
        "            in_channels=self.out_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.w_c_i = Parameter(torch.Tensor(1, self.out_channels))\n",
        "        self.b_i = Parameter(torch.Tensor(1, self.out_channels))\n",
        "\n",
        "    def _create_forget_gate_parameters_and_layers(self):\n",
        "\n",
        "        self.conv_x_f = ChebConv(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.conv_h_f = ChebConv(\n",
        "            in_channels=self.out_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.w_c_f = Parameter(torch.Tensor(1, self.out_channels))\n",
        "        self.b_f = Parameter(torch.Tensor(1, self.out_channels))\n",
        "\n",
        "    def _create_cell_state_parameters_and_layers(self):\n",
        "\n",
        "        self.conv_x_c = ChebConv(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.conv_h_c = ChebConv(\n",
        "            in_channels=self.out_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.b_c = Parameter(torch.Tensor(1, self.out_channels))\n",
        "\n",
        "    def _create_output_gate_parameters_and_layers(self):\n",
        "\n",
        "        self.conv_x_o = ChebConv(\n",
        "            in_channels=self.in_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.conv_h_o = ChebConv(\n",
        "            in_channels=self.out_channels,\n",
        "            out_channels=self.out_channels,\n",
        "            K=self.K,\n",
        "            normalization=self.normalization,\n",
        "            bias=self.bias,\n",
        "        )\n",
        "\n",
        "        self.w_c_o = Parameter(torch.Tensor(1, self.out_channels))\n",
        "        self.b_o = Parameter(torch.Tensor(1, self.out_channels))\n",
        "\n",
        "    def _create_parameters_and_layers(self):\n",
        "        self._create_input_gate_parameters_and_layers()\n",
        "        self._create_forget_gate_parameters_and_layers()\n",
        "        self._create_cell_state_parameters_and_layers()\n",
        "        self._create_output_gate_parameters_and_layers()\n",
        "\n",
        "    def _set_parameters(self):\n",
        "        glorot(self.w_c_i)\n",
        "        glorot(self.w_c_f)\n",
        "        glorot(self.w_c_o)\n",
        "        zeros(self.b_i)\n",
        "        zeros(self.b_f)\n",
        "        zeros(self.b_c)\n",
        "        zeros(self.b_o)\n",
        "\n",
        "    def _set_hidden_state(self, X, H):\n",
        "        if H is None:\n",
        "            H = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n",
        "        return H\n",
        "\n",
        "    def _set_cell_state(self, X, C):\n",
        "        if C is None:\n",
        "            C = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n",
        "        return C\n",
        "\n",
        "    def _calculate_input_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
        "        I = self.conv_x_i(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        I = I + self.conv_h_i(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        I = I + (self.w_c_i * C)\n",
        "        I = I + self.b_i\n",
        "        I = torch.sigmoid(I)\n",
        "        return I\n",
        "\n",
        "    def _calculate_forget_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
        "        F = self.conv_x_f(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        F = F + self.conv_h_f(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        F = F + (self.w_c_f * C)\n",
        "        F = F + self.b_f\n",
        "        F = torch.sigmoid(F)\n",
        "        return F\n",
        "\n",
        "    def _calculate_cell_state(self, X, edge_index, edge_weight, H, C, I, F, lambda_max):\n",
        "        T = self.conv_x_c(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        T = T + self.conv_h_c(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        T = T + self.b_c\n",
        "        T = torch.tanh(T)\n",
        "        C = F * C + I * T\n",
        "        return C\n",
        "\n",
        "    def _calculate_output_gate(self, X, edge_index, edge_weight, H, C, lambda_max):\n",
        "        O = self.conv_x_o(X, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        O = O + self.conv_h_o(H, edge_index, edge_weight, lambda_max=lambda_max)\n",
        "        O = O + (self.w_c_o * C)\n",
        "        O = O + self.b_o\n",
        "        O = torch.sigmoid(O)\n",
        "        return O\n",
        "\n",
        "    def _calculate_hidden_state(self, O, C):\n",
        "        H = O * torch.tanh(C)\n",
        "        return H\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        X: torch.FloatTensor,\n",
        "        edge_index: torch.LongTensor,\n",
        "        edge_weight: torch.FloatTensor = None,\n",
        "        H: torch.FloatTensor = None,\n",
        "        C: torch.FloatTensor = None,\n",
        "        lambda_max: torch.Tensor = None,\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        Making a forward pass. If edge weights are not present the forward pass\n",
        "        defaults to an unweighted graph. If the hidden state and cell state\n",
        "        matrices are not present when the forward pass is called these are\n",
        "        initialized with zeros.\n",
        "\n",
        "        Arg types:\n",
        "            * **X** *(PyTorch Float Tensor)* - Node features.\n",
        "            * **edge_index** *(PyTorch Long Tensor)* - Graph edge indices.\n",
        "            * **edge_weight** *(PyTorch Long Tensor, optional)* - Edge weight vector.\n",
        "            * **H** *(PyTorch Float Tensor, optional)* - Hidden state matrix for all nodes.\n",
        "            * **C** *(PyTorch Float Tensor, optional)* - Cell state matrix for all nodes.\n",
        "            * **lambda_max** *(PyTorch Tensor, optional but mandatory if normalization is not sym)* - Largest eigenvalue of Laplacian.\n",
        "\n",
        "        Return types:\n",
        "            * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\n",
        "            * **C** *(PyTorch Float Tensor)* - Cell state matrix for all nodes.\n",
        "        \"\"\"\n",
        "        H = self._set_hidden_state(X, H)\n",
        "        C = self._set_cell_state(X, C)\n",
        "        I = self._calculate_input_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
        "        F = self._calculate_forget_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
        "        C = self._calculate_cell_state(X, edge_index, edge_weight, H, C, I, F, lambda_max)\n",
        "        O = self._calculate_output_gate(X, edge_index, edge_weight, H, C, lambda_max)\n",
        "        H = self._calculate_hidden_state(O, C)\n",
        "        return H, C"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d94cd12c",
      "metadata": {
        "id": "d94cd12c"
      },
      "source": [
        "## Temporal link predictor model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rVlPlbnt5GIB",
      "metadata": {
        "id": "rVlPlbnt5GIB"
      },
      "outputs": [],
      "source": [
        "class TemporalGNN(torch.nn.Module):\n",
        "    def __init__(self, num_nodes, node_features, hidden_channels, output_channels):\n",
        "        super(TemporalGNN, self).__init__()\n",
        "        self.recurrent = GConvLSTM(node_features, hidden_channels, 3)\n",
        "        self.linear = torch.nn.Linear(hidden_channels, output_channels)\n",
        "        self.edge_mlp = torch.nn.Sequential(\n",
        "                torch.nn.Linear(2 * output_channels, hidden_channels),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Linear(hidden_channels, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, seq):\n",
        "        H, C = None, None\n",
        "        for i in range(len(seq)):\n",
        "            x = seq[i].x\n",
        "            edge_index = seq[i].edge_index\n",
        "            edge_attr = seq[i].edge_attr\n",
        "            H, C = self.recurrent(x, edge_index, edge_attr, H, C)\n",
        "\n",
        "        H = F.relu(H)\n",
        "        H = self.linear(H)\n",
        "        return F.log_softmax(H, dim=1)\n",
        "\n",
        "    def predict_edge_weight(self, node_embeddings, edge_index):\n",
        "        src, dst = edge_index\n",
        "        edge_features = torch.cat([node_embeddings[src], node_embeddings[dst]], dim=1)\n",
        "        probs = self.edge_mlp(edge_features)\n",
        "        probs = torch.sigmoid(probs)\n",
        "        return probs.squeeze()\n",
        "\n",
        "    def get_edge_embeddings(self, node_embeddings, edge_index):\n",
        "        src, dst = edge_index\n",
        "        edge_features = torch.cat([node_embeddings[src], node_embeddings[dst]], dim=1)\n",
        "        return edge_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "187b6576",
      "metadata": {
        "id": "187b6576"
      },
      "source": [
        "# Initializing model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b4b29a-b6e8-46e6-9cf1-876266b61970",
      "metadata": {
        "id": "c9b4b29a-b6e8-46e6-9cf1-876266b61970"
      },
      "outputs": [],
      "source": [
        "node_dim = graphs[0].x.shape[1]\n",
        "num_nodes = graphs[0].x.shape[0]\n",
        "hidden_channels = 64\n",
        "output_channels = 64\n",
        "learning_rate = 0.0001\n",
        "epochs = 30\n",
        "time_window = 10\n",
        "weight_decay = 0.0001\n",
        "\n",
        "tgn_model = TemporalGNN(num_nodes, node_dim, hidden_channels, output_channels)\n",
        "optimizer = torch.optim.Adam(tgn_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "criterion = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gcsfs\n",
        "\n",
        "path = 'gs://datasets-dev-ded86f66/benchmarks/scientific_trend_prediction/model_weights/TGN_90th_precentile_checkpoint.pth'\n",
        "fs = gcsfs.GCSFileSystem()\n",
        "\n",
        "with fs.open(path, 'rb') as f:\n",
        "    state_dict = torch.load(f)\n",
        "\n",
        "tgn_model.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "guiMqoWTFqfJ"
      },
      "id": "guiMqoWTFqfJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1155e74b-3c46-46ef-b7c0-0c9e8df43564",
      "metadata": {
        "id": "1155e74b-3c46-46ef-b7c0-0c9e8df43564"
      },
      "source": [
        "## Creating an 80:20 train-test split, where the test data follows the training data in chronological order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78df504e-a7ef-4e2e-be78-dc7718ca8e92",
      "metadata": {
        "id": "78df504e-a7ef-4e2e-be78-dc7718ca8e92"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "\n",
        "def create_sequences(data, time_step):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - time_step - 1):\n",
        "        X.append(data[i:(i + time_step)])\n",
        "        Y.append(data[i + time_step])\n",
        "    return X, Y\n",
        "\n",
        "x, y = create_sequences(graphs, time_window)\n",
        "\n",
        "split_index = int(len(x) * 0.8)\n",
        "\n",
        "x_train, x_test = copy.deepcopy(x[:split_index]), copy.deepcopy(x[split_index:])\n",
        "y_train, y_test = copy.deepcopy(y[:split_index]), copy.deepcopy(y[split_index:])\n",
        "\n",
        "print(\"Size of x_train:\", len(x_train))\n",
        "print(\"Size of x_test:\", len(x_test))\n",
        "print(\"Size of y_train:\", len(y_train))\n",
        "print(\"Size of y_test:\", len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "def create_sequences(data, time_step, forecast_length):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(data) - time_step - forecast_length + 1):\n",
        "        X.append(data[i:i + time_step])\n",
        "        Y.append(data[i + time_step + forecast_length - 1])\n",
        "    return X, Y\n",
        "\n",
        "time_window = 10\n",
        "forecast_length = 5\n",
        "\n",
        "x, y = create_sequences(graphs, time_window, forecast_length)\n",
        "\n",
        "split_index = int(len(x) * 0.8)\n",
        "\n",
        "x_train, x_test = copy.deepcopy(x[:split_index]), copy.deepcopy(x[split_index:])\n",
        "y_train, y_test = copy.deepcopy(y[:split_index]), copy.deepcopy(y[split_index:])\n",
        "\n",
        "print(\"Size of x_train:\", len(x_train))\n",
        "print(\"Size of x_test:\", len(x_test))\n",
        "print(\"Size of y_train:\", len(y_train))\n",
        "print(\"Size of y_test:\", len(y_test))"
      ],
      "metadata": {
        "id": "TTdQ55-5ISE6"
      },
      "id": "TTdQ55-5ISE6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tgn_model.load_state_dict(torch.load('test_model.pth'))"
      ],
      "metadata": {
        "id": "vuq6JPbQIUMV"
      },
      "id": "vuq6JPbQIUMV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bf820c34-b62c-4619-9299-f3d3373ced0e",
      "metadata": {
        "id": "bf820c34-b62c-4619-9299-f3d3373ced0e"
      },
      "source": [
        "## Generating negative samples equal to the number of positive samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dac0b90-096d-4529-944e-16ddca73fa56",
      "metadata": {
        "id": "9dac0b90-096d-4529-944e-16ddca73fa56"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "def add_negative_samples(data):\n",
        "    num_pos_samples = data.edge_index.size(1)\n",
        "    num_neg_samples = num_pos_samples\n",
        "    neg_edge_index = negative_sampling(data.edge_index, num_nodes=data.num_nodes, num_neg_samples=num_neg_samples)\n",
        "    neg_weights = torch.zeros(num_neg_samples, device=data.edge_index.device)\n",
        "\n",
        "    data.edge_index = torch.cat([data.edge_index, neg_edge_index], dim=1)\n",
        "    data.y = torch.cat([data.edge_attr, neg_weights])\n",
        "    data.edge_attr = data.y\n",
        "\n",
        "    perm = torch.randperm(data.edge_index.size(1))\n",
        "\n",
        "    data.edge_index = data.edge_index[:, perm]\n",
        "    data.edge_attr = data.edge_attr[perm]\n",
        "    data.y = data.y[perm]\n",
        "\n",
        "    return data\n",
        "\n",
        "for i in range(len(y_train)):\n",
        "    y_train[i] = add_negative_samples(y_train[i])\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    y_test[i] = add_negative_samples(y_test[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eff7b51-e82d-4be9-941e-39dee26efc4e",
      "metadata": {
        "id": "0eff7b51-e82d-4be9-941e-39dee26efc4e"
      },
      "source": [
        "## Training loop for TGN with objective to predict edges for N+1th graph given last N graphs. Note: Use saved model: 'TGN_90th_precentile_checkpoint.pth' to save time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a551bd8-44b4-495f-b587-cbc84fac52a0",
      "metadata": {
        "id": "8a551bd8-44b4-495f-b587-cbc84fac52a0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "tgn_model.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    tgn_model.train()\n",
        "    for i in range(len(x_train)):\n",
        "        optimizer.zero_grad()\n",
        "        node_embeddings = tgn_model(x_train[i])\n",
        "        probs = tgn_model.predict_edge_weight(node_embeddings, y_train[i].edge_index)\n",
        "        loss = criterion(probs, y_train[i].y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(x_train)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    tgn_model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(x_test)):\n",
        "            node_embeddings = tgn_model(x_test[i])\n",
        "            val_loss = criterion(weights, y_test[i].y)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(x_test)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {avg_train_loss}, Val Loss: {avg_val_loss}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c6c55c-7d47-4c7d-ad8d-45c91b425b44",
      "metadata": {
        "id": "83c6c55c-7d47-4c7d-ad8d-45c91b425b44"
      },
      "outputs": [],
      "source": [
        "#torch.save(tgn_model.state_dict(), 'TGN_90th_precentile_checkpoint.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33f06f45-9895-49ca-8ae1-ccb929cf8330",
      "metadata": {
        "id": "33f06f45-9895-49ca-8ae1-ccb929cf8330"
      },
      "source": [
        "## Decoupled MLP with inmemory TGN embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd6d17a1-8ce9-4a88-87b5-79aa555238f4",
      "metadata": {
        "id": "dd6d17a1-8ce9-4a88-87b5-79aa555238f4"
      },
      "outputs": [],
      "source": [
        "class EdgeMLP(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(EdgeMLP, self).__init__()\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, hidden_dim),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, edge_features):\n",
        "        probs = self.mlp(edge_features)\n",
        "        probs = torch.sigmoid(probs)\n",
        "        return probs.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c124c782-44d3-4533-930d-927bea416361",
      "metadata": {
        "id": "c124c782-44d3-4533-930d-927bea416361"
      },
      "outputs": [],
      "source": [
        "def extract_edge_embeddings(graphs,edge_index):\n",
        "    node_embeddings = tgn_model(graphs)\n",
        "    edge_embeddings = tgn_model.get_edge_embeddings(node_embeddings,edge_index).detach().numpy()\n",
        "    return edge_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e13981-683b-4baf-b3a0-81e61a7b6863",
      "metadata": {
        "id": "98e13981-683b-4baf-b3a0-81e61a7b6863"
      },
      "outputs": [],
      "source": [
        "x_train_mlp = []\n",
        "y_train_mlp = []\n",
        "\n",
        "for i in range(len(x_train)):\n",
        "    edge_embs = extract_edge_embeddings(x_train[i],y_train[i].edge_index)\n",
        "    x_train_mlp.append(edge_embs)\n",
        "    y_train_mlp.append(y_train[i].y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6092a10a-9a7f-4702-bd8e-69448f140450",
      "metadata": {
        "id": "6092a10a-9a7f-4702-bd8e-69448f140450"
      },
      "outputs": [],
      "source": [
        "x_test_mlp = []\n",
        "y_test_mlp = []\n",
        "\n",
        "for i in range(len(x_test)):\n",
        "    edge_embs = extract_edge_embeddings(x_test[i],y_test[i].edge_index)\n",
        "    x_test_mlp.append(edge_embs)\n",
        "    y_test_mlp.append(y_test[i].y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad3b830f-57fd-4ae2-a654-2d350d797d05",
      "metadata": {
        "id": "ad3b830f-57fd-4ae2-a654-2d350d797d05"
      },
      "outputs": [],
      "source": [
        "input_dim = x_train_mlp[0].shape[1]\n",
        "hidden_dim = 64\n",
        "mlp_model = EdgeMLP(input_dim, hidden_dim)\n",
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.0001)\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    mlp_model.train()\n",
        "    train_loss_total = 0.0\n",
        "    for i in range(len(x_train_mlp)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = mlp_model(torch.tensor(x_train_mlp[i]).float()).squeeze()\n",
        "        loss = criterion(outputs, torch.tensor(y_train_mlp[i]).float())\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_total += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss_total / len(x_train_mlp)\n",
        "\n",
        "    mlp_model.eval()\n",
        "    val_loss_total = 0.0\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(x_test_mlp)):\n",
        "            outputs = mlp_model(torch.tensor(x_test_mlp[i]).float()).squeeze()\n",
        "            loss = criterion(outputs, torch.tensor(y_test_mlp[i]).float())\n",
        "            val_loss_total += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss_total / len(x_test_mlp)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4aa9fae-068c-4545-aae7-31571480e8ec",
      "metadata": {
        "id": "c4aa9fae-068c-4545-aae7-31571480e8ec"
      },
      "source": [
        "## Decoupled LSTM with in memory TGN embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35280b15-0848-4d71-946f-68df3bb0c601",
      "metadata": {
        "id": "35280b15-0848-4d71-946f-68df3bb0c601"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "437bbf09-db39-4106-a98e-b88a32cc4880",
      "metadata": {
        "id": "437bbf09-db39-4106-a98e-b88a32cc4880"
      },
      "outputs": [],
      "source": [
        "def extract_edge_embeddings(graphs,edge_index):\n",
        "    node_embeddings = tgn_model(graphs)\n",
        "    edge_embeddings = tgn_model.get_edge_embeddings(node_embeddings,edge_index).detach().numpy()\n",
        "    return edge_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "461a7c32-381f-4dc4-a9cc-57a9832f1ba6",
      "metadata": {
        "id": "461a7c32-381f-4dc4-a9cc-57a9832f1ba6"
      },
      "outputs": [],
      "source": [
        "x_train_lstm = []\n",
        "y_train_lstm = []\n",
        "\n",
        "for i in range(len(x_train)):\n",
        "    embs = []\n",
        "    for j in range(len(x_train[i])):\n",
        "        edge_embs = extract_edge_embeddings([x_train[i][j]],y_train[i].edge_index)\n",
        "        embs.append(edge_embs)\n",
        "    stacked_embs = np.stack(embs,axis=1)\n",
        "    x_train_lstm.append(stacked_embs)\n",
        "    y_train_lstm.append(y_train[i].y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d50bf603-9ed2-459a-9c75-cab3b0d43a17",
      "metadata": {
        "id": "d50bf603-9ed2-459a-9c75-cab3b0d43a17"
      },
      "outputs": [],
      "source": [
        "x_train_lstm = np.vstack(x_train_lstm)\n",
        "y_train_lstm = np.hstack(y_train_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77120a5c-4304-44cd-a696-4d119c47df3e",
      "metadata": {
        "id": "77120a5c-4304-44cd-a696-4d119c47df3e"
      },
      "outputs": [],
      "source": [
        "x_test_lstm = []\n",
        "y_test_lstm = []\n",
        "\n",
        "for i in range(len(x_test)):\n",
        "    embs = []\n",
        "    for j in range(len(x_test[i])):\n",
        "        edge_embs = extract_edge_embeddings([x_test[i][j]],y_test[i].edge_index)\n",
        "        embs.append(edge_embs)\n",
        "    stacked_embs = np.stack(embs,axis=1)\n",
        "    x_test_lstm.append(stacked_embs)\n",
        "    y_test_lstm.append(y_test[i].y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0132c01a-e66c-41f7-b232-3921d3e3117f",
      "metadata": {
        "id": "0132c01a-e66c-41f7-b232-3921d3e3117f"
      },
      "outputs": [],
      "source": [
        "x_test_lstm = np.vstack(x_test_lstm)\n",
        "y_test_lstm = np.hstack(y_test_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d45e2221-db5e-4753-b0e0-86d5833fde8e",
      "metadata": {
        "id": "d45e2221-db5e-4753-b0e0-86d5833fde8e"
      },
      "outputs": [],
      "source": [
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(50, return_sequences=True, input_shape=(x_train_lstm.shape[1], x_train_lstm.shape[2])))\n",
        "lstm_model.add(LSTM(50))\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "history = lstm_model.fit(x_train_lstm, y_train_lstm, batch_size=1024, epochs=3, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8638b629-0407-4add-884d-d52ec8e711c2",
      "metadata": {
        "id": "8638b629-0407-4add-884d-d52ec8e711c2"
      },
      "outputs": [],
      "source": [
        "y_pred = lstm_model.predict(x_test_lstm,batch_size = 1024, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d6e721c-b4a0-439f-a188-5c5c1c6cce36",
      "metadata": {
        "id": "7d6e721c-b4a0-439f-a188-5c5c1c6cce36"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "mse = mean_squared_error(y_test_lstm, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qbwb11aoIdo1"
      },
      "id": "qbwb11aoIdo1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "RMSE_Decoupled_MLP_LSTM_Inmemory_TGN_embeddings_demo.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}